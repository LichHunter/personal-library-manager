{
  "metadata": {
    "timestamp": "2026-01-25_155732",
    "num_documents": 5,
    "num_queries": 5,
    "total_facts": 14,
    "has_human_queries": true
  },
  "evaluations": [
    {
      "strategy": "bmx_pure",
      "chunking": "fixed_512_0pct",
      "embedder": "BAAI/bge-base-en-v1.5",
      "reranker": null,
      "llm": null,
      "k": 5,
      "metric": "exact_match",
      "num_chunks": 51,
      "index_time_s": 2.1915603300003568,
      "aggregate": {
        "original": {
          "coverage": 0.7857142857142857,
          "found": 11,
          "total": 14,
          "avg_latency_ms": 147.7274797995051,
          "p95_latency_ms": 543.7193565987399
        },
        "synonym": {
          "coverage": 0.7857142857142857,
          "found": 11,
          "total": 14,
          "avg_latency_ms": 13.907450199621962,
          "p95_latency_ms": 15.492716799053596
        },
        "problem": {
          "coverage": 0.7142857142857143,
          "found": 10,
          "total": 14,
          "avg_latency_ms": 14.939976399909938,
          "p95_latency_ms": 15.650244599964935
        },
        "casual": {
          "coverage": 0.42857142857142855,
          "found": 6,
          "total": 14,
          "avg_latency_ms": 14.916871399327647,
          "p95_latency_ms": 15.443637000134913
        },
        "contextual": {
          "coverage": 0.7857142857142857,
          "found": 11,
          "total": 14,
          "avg_latency_ms": 15.093575000355486,
          "p95_latency_ms": 15.699235801002942
        },
        "negation": {
          "coverage": 0.7142857142857143,
          "found": 10,
          "total": 14,
          "avg_latency_ms": 15.758273800020106,
          "p95_latency_ms": 17.81962920067599
        }
      },
      "per_query": [
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 674.8775749983906,
          "query_id": "realistic_001",
          "dimension": "original",
          "query": "What is the API rate limit per minute?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 9.792299999389797,
          "query_id": "realistic_001",
          "dimension": "synonym",
          "query": "How many requests can I make per minute to the API?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 14.123371998721268,
          "query_id": "realistic_001",
          "dimension": "problem",
          "query": "My API calls are getting blocked, what's the limit?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 14.315961998363491,
          "query_id": "realistic_001",
          "dimension": "casual",
          "query": "api rate limit"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 14.293880998593522,
          "query_id": "realistic_001",
          "dimension": "contextual",
          "query": "I'm building a batch job that calls CloudFlow API repeatedly, what rate limit should I expect?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 14.27841199983959,
          "query_id": "realistic_001",
          "dimension": "negation",
          "query": "Why is the API rejecting my requests after 100 calls?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            }
          ],
          "latency_ms": 13.897311000619084,
          "query_id": "realistic_002",
          "dimension": "original",
          "query": "How do I fix 429 Too Many Requests errors?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 13.823242999933427,
          "query_id": "realistic_002",
          "dimension": "synonym",
          "query": "What should I do when I get rate limited?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 14.416359001188539,
          "query_id": "realistic_002",
          "dimension": "problem",
          "query": "My requests keep failing with 429 status code"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 14.783171998715261,
          "query_id": "realistic_002",
          "dimension": "casual",
          "query": "429 error fix"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            }
          ],
          "latency_ms": 15.555031000985764,
          "query_id": "realistic_002",
          "dimension": "contextual",
          "query": "I'm getting throttled by CloudFlow API, how can I handle this in my application?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            }
          ],
          "latency_ms": 15.486873999179807,
          "query_id": "realistic_002",
          "dimension": "negation",
          "query": "Why am I being blocked with rate limit errors?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            }
          ],
          "latency_ms": 15.369245000329101,
          "query_id": "realistic_003",
          "dimension": "original",
          "query": "What is the JWT token expiration time?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 15.466355998796644,
          "query_id": "realistic_003",
          "dimension": "synonym",
          "query": "How long do access tokens last?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            }
          ],
          "latency_ms": 15.274247001798358,
          "query_id": "realistic_003",
          "dimension": "problem",
          "query": "My authentication keeps expiring after an hour"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [],
          "missed_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 15.250332999130478,
          "query_id": "realistic_003",
          "dimension": "casual",
          "query": "token expiry time"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            }
          ],
          "latency_ms": 15.735287001007237,
          "query_id": "realistic_003",
          "dimension": "contextual",
          "query": "I need to implement token refresh logic, when do JWT tokens expire?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            }
          ],
          "latency_ms": 18.402818001050036,
          "query_id": "realistic_003",
          "dimension": "negation",
          "query": "Why does my token stop working after 3600 seconds?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            }
          ],
          "latency_ms": 19.086483000137378,
          "query_id": "realistic_004",
          "dimension": "original",
          "query": "What database technology does CloudFlow use?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            }
          ],
          "latency_ms": 15.499306999117834,
          "query_id": "realistic_004",
          "dimension": "synonym",
          "query": "Which database system powers CloudFlow?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            }
          ],
          "latency_ms": 15.74424399950658,
          "query_id": "realistic_004",
          "dimension": "problem",
          "query": "I need to understand the data storage architecture"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 15.491963000386022,
          "query_id": "realistic_004",
          "dimension": "casual",
          "query": "database stack"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 14.70392500050366,
          "query_id": "realistic_004",
          "dimension": "contextual",
          "query": "For capacity planning, what databases does CloudFlow rely on?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            }
          ],
          "latency_ms": 15.306558001611847,
          "query_id": "realistic_004",
          "dimension": "negation",
          "query": "Is CloudFlow using MySQL or something else?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            }
          ],
          "latency_ms": 15.406784998049261,
          "query_id": "realistic_005",
          "dimension": "original",
          "query": "What is the Kubernetes namespace for production deployment?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            }
          ],
          "latency_ms": 14.956045000872109,
          "query_id": "realistic_005",
          "dimension": "synonym",
          "query": "Which namespace is used for CloudFlow production?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [],
          "missed_facts": [
            "cloudflow-prod"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            }
          ],
          "latency_ms": 15.141659998334944,
          "query_id": "realistic_005",
          "dimension": "problem",
          "query": "I can't find the production pods"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            }
          ],
          "latency_ms": 14.742927000042982,
          "query_id": "realistic_005",
          "dimension": "casual",
          "query": "prod namespace"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            }
          ],
          "latency_ms": 15.179751000687247,
          "query_id": "realistic_005",
          "dimension": "contextual",
          "query": "I need to deploy to production, what namespace should I use?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            }
          ],
          "latency_ms": 15.31670699841925,
          "query_id": "realistic_005",
          "dimension": "negation",
          "query": "Why can't I see resources in the default namespace?"
        }
      ]
    },
    {
      "strategy": "bmx_wqa",
      "chunking": "fixed_512_0pct",
      "embedder": "BAAI/bge-base-en-v1.5",
      "reranker": null,
      "llm": "fast",
      "k": 5,
      "metric": "exact_match",
      "num_chunks": 51,
      "index_time_s": 1.7529065569979139,
      "aggregate": {
        "original": {
          "coverage": 0.7857142857142857,
          "found": 11,
          "total": 14,
          "avg_latency_ms": 1433.7675838003634,
          "p95_latency_ms": 1674.64905980014
        },
        "synonym": {
          "coverage": 0.7857142857142857,
          "found": 11,
          "total": 14,
          "avg_latency_ms": 1419.5345984000596,
          "p95_latency_ms": 1637.4615602013364
        },
        "problem": {
          "coverage": 0.7142857142857143,
          "found": 10,
          "total": 14,
          "avg_latency_ms": 1401.0306675983884,
          "p95_latency_ms": 1549.143233399809
        },
        "casual": {
          "coverage": 0.42857142857142855,
          "found": 6,
          "total": 14,
          "avg_latency_ms": 1398.833602200466,
          "p95_latency_ms": 1523.847231001855
        },
        "contextual": {
          "coverage": 0.7857142857142857,
          "found": 11,
          "total": 14,
          "avg_latency_ms": 1568.7883791993954,
          "p95_latency_ms": 1821.7224071995588
        },
        "negation": {
          "coverage": 0.7142857142857143,
          "found": 10,
          "total": 14,
          "avg_latency_ms": 1618.3671293998486,
          "p95_latency_ms": 2119.523468999978
        }
      },
      "per_query": [
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 1344.3686270002217,
          "query_id": "realistic_001",
          "dimension": "original",
          "query": "What is the API rate limit per minute?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 1348.7700100013171,
          "query_id": "realistic_001",
          "dimension": "synonym",
          "query": "How many requests can I make per minute to the API?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 1357.3239219986135,
          "query_id": "realistic_001",
          "dimension": "problem",
          "query": "My API calls are getting blocked, what's the limit?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 1376.1029199995392,
          "query_id": "realistic_001",
          "dimension": "casual",
          "query": "api rate limit"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 1325.5866699983017,
          "query_id": "realistic_001",
          "dimension": "contextual",
          "query": "I'm building a batch job that calls CloudFlow API repeatedly, what rate limit should I expect?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 1326.7736109992256,
          "query_id": "realistic_001",
          "dimension": "negation",
          "query": "Why is the API rejecting my requests after 100 calls?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            }
          ],
          "latency_ms": 1722.438916000101,
          "query_id": "realistic_002",
          "dimension": "original",
          "query": "How do I fix 429 Too Many Requests errors?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 1660.8340080019843,
          "query_id": "realistic_002",
          "dimension": "synonym",
          "query": "What should I do when I get rate limited?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 1388.912218997575,
          "query_id": "realistic_002",
          "dimension": "problem",
          "query": "My requests keep failing with 429 status code"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 1527.931446002185,
          "query_id": "realistic_002",
          "dimension": "casual",
          "query": "429 error fix"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            }
          ],
          "latency_ms": 1753.5900479997508,
          "query_id": "realistic_002",
          "dimension": "contextual",
          "query": "I'm getting throttled by CloudFlow API, how can I handle this in my application?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            }
          ],
          "latency_ms": 1711.2930569965101,
          "query_id": "realistic_002",
          "dimension": "negation",
          "query": "Why am I being blocked with rate limit errors?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            }
          ],
          "latency_ms": 1483.4896350002964,
          "query_id": "realistic_003",
          "dimension": "original",
          "query": "What is the JWT token expiration time?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 1543.9717689987447,
          "query_id": "realistic_003",
          "dimension": "synonym",
          "query": "How long do access tokens last?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            }
          ],
          "latency_ms": 1320.5927229973895,
          "query_id": "realistic_003",
          "dimension": "problem",
          "query": "My authentication keeps expiring after an hour"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [],
          "missed_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 1507.510371000535,
          "query_id": "realistic_003",
          "dimension": "casual",
          "query": "token expiry time"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            }
          ],
          "latency_ms": 1403.324884999165,
          "query_id": "realistic_003",
          "dimension": "contextual",
          "query": "I need to implement token refresh logic, when do JWT tokens expire?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            }
          ],
          "latency_ms": 2221.581072000845,
          "query_id": "realistic_003",
          "dimension": "negation",
          "query": "Why does my token stop working after 3600 seconds?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            }
          ],
          "latency_ms": 1373.6166740018234,
          "query_id": "realistic_004",
          "dimension": "original",
          "query": "What database technology does CloudFlow use?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            }
          ],
          "latency_ms": 1322.1895749993564,
          "query_id": "realistic_004",
          "dimension": "synonym",
          "query": "Which database system powers CloudFlow?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            }
          ],
          "latency_ms": 1589.2009870003676,
          "query_id": "realistic_004",
          "dimension": "problem",
          "query": "I need to understand the data storage architecture"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 1294.1821639979025,
          "query_id": "realistic_004",
          "dimension": "casual",
          "query": "database stack"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 1522.6847960002488,
          "query_id": "realistic_004",
          "dimension": "contextual",
          "query": "For capacity planning, what databases does CloudFlow rely on?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            }
          ],
          "latency_ms": 1537.222950002615,
          "query_id": "realistic_004",
          "dimension": "negation",
          "query": "Is CloudFlow using MySQL or something else?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            }
          ],
          "latency_ms": 1244.9240669993742,
          "query_id": "realistic_005",
          "dimension": "original",
          "query": "What is the Kubernetes namespace for production deployment?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            }
          ],
          "latency_ms": 1221.9076299988956,
          "query_id": "realistic_005",
          "dimension": "synonym",
          "query": "Which namespace is used for CloudFlow production?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [],
          "missed_facts": [
            "cloudflow-prod"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            }
          ],
          "latency_ms": 1349.1234869979962,
          "query_id": "realistic_005",
          "dimension": "problem",
          "query": "I can't find the production pods"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            }
          ],
          "latency_ms": 1288.441110002168,
          "query_id": "realistic_005",
          "dimension": "casual",
          "query": "prod namespace"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            }
          ],
          "latency_ms": 1838.7554969995108,
          "query_id": "realistic_005",
          "dimension": "contextual",
          "query": "I need to deploy to production, what namespace should I use?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            }
          ],
          "latency_ms": 1294.9649570000474,
          "query_id": "realistic_005",
          "dimension": "negation",
          "query": "Why can't I see resources in the default namespace?"
        }
      ]
    },
    {
      "strategy": "bmx_semantic",
      "chunking": "fixed_512_0pct",
      "embedder": "BAAI/bge-base-en-v1.5",
      "reranker": null,
      "llm": null,
      "k": 5,
      "metric": "exact_match",
      "num_chunks": 51,
      "index_time_s": 1.7610871489996498,
      "aggregate": {
        "original": {
          "coverage": 0.7857142857142857,
          "found": 11,
          "total": 14,
          "avg_latency_ms": 14.927574598550564,
          "p95_latency_ms": 15.631011798541294
        },
        "synonym": {
          "coverage": 0.7857142857142857,
          "found": 11,
          "total": 14,
          "avg_latency_ms": 15.759014600189403,
          "p95_latency_ms": 17.14348460009205
        },
        "problem": {
          "coverage": 0.7142857142857143,
          "found": 10,
          "total": 14,
          "avg_latency_ms": 15.408371399098542,
          "p95_latency_ms": 15.815372198994737
        },
        "casual": {
          "coverage": 0.42857142857142855,
          "found": 6,
          "total": 14,
          "avg_latency_ms": 15.195744999800809,
          "p95_latency_ms": 15.558996598701924
        },
        "contextual": {
          "coverage": 0.7857142857142857,
          "found": 11,
          "total": 14,
          "avg_latency_ms": 15.55161619980936,
          "p95_latency_ms": 16.112718399381265
        },
        "negation": {
          "coverage": 0.7142857142857143,
          "found": 10,
          "total": 14,
          "avg_latency_ms": 15.386840998689877,
          "p95_latency_ms": 15.749560999392997
        }
      },
      "per_query": [
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 13.154074997146381,
          "query_id": "realistic_001",
          "dimension": "original",
          "query": "What is the API rate limit per minute?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 17.577339000126813,
          "query_id": "realistic_001",
          "dimension": "synonym",
          "query": "How many requests can I make per minute to the API?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 15.874856999289477,
          "query_id": "realistic_001",
          "dimension": "problem",
          "query": "My API calls are getting blocked, what's the limit?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 15.595125998515869,
          "query_id": "realistic_001",
          "dimension": "casual",
          "query": "api rate limit"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 16.255075999652036,
          "query_id": "realistic_001",
          "dimension": "contextual",
          "query": "I'm building a batch job that calls CloudFlow API repeatedly, what rate limit should I expect?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 15.674412999942433,
          "query_id": "realistic_001",
          "dimension": "negation",
          "query": "Why is the API rejecting my requests after 100 calls?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            }
          ],
          "latency_ms": 15.10649400006514,
          "query_id": "realistic_002",
          "dimension": "original",
          "query": "How do I fix 429 Too Many Requests errors?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 15.302690000680741,
          "query_id": "realistic_002",
          "dimension": "synonym",
          "query": "What should I do when I get rate limited?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 15.34113199886633,
          "query_id": "realistic_002",
          "dimension": "problem",
          "query": "My requests keep failing with 429 status code"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 15.414478999446146,
          "query_id": "realistic_002",
          "dimension": "casual",
          "query": "429 error fix"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            }
          ],
          "latency_ms": 15.485741998418234,
          "query_id": "realistic_002",
          "dimension": "contextual",
          "query": "I'm getting throttled by CloudFlow API, how can I handle this in my application?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            }
          ],
          "latency_ms": 15.32456099812407,
          "query_id": "realistic_002",
          "dimension": "negation",
          "query": "Why am I being blocked with rate limit errors?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            }
          ],
          "latency_ms": 15.696032998675946,
          "query_id": "realistic_003",
          "dimension": "original",
          "query": "What is the JWT token expiration time?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 15.263977998984046,
          "query_id": "realistic_003",
          "dimension": "synonym",
          "query": "How long do access tokens last?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            }
          ],
          "latency_ms": 15.577432997815777,
          "query_id": "realistic_003",
          "dimension": "problem",
          "query": "My authentication keeps expiring after an hour"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [],
          "missed_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 14.986341000621906,
          "query_id": "realistic_003",
          "dimension": "casual",
          "query": "token expiry time"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            }
          ],
          "latency_ms": 15.34747299956507,
          "query_id": "realistic_003",
          "dimension": "contextual",
          "query": "I need to implement token refresh logic, when do JWT tokens expire?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            }
          ],
          "latency_ms": 15.768347999255639,
          "query_id": "realistic_003",
          "dimension": "negation",
          "query": "Why does my token stop working after 3600 seconds?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            }
          ],
          "latency_ms": 15.310343998862663,
          "query_id": "realistic_004",
          "dimension": "original",
          "query": "What database technology does CloudFlow use?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            }
          ],
          "latency_ms": 15.408066999953007,
          "query_id": "realistic_004",
          "dimension": "synonym",
          "query": "Which database system powers CloudFlow?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            }
          ],
          "latency_ms": 15.327896999224322,
          "query_id": "realistic_004",
          "dimension": "problem",
          "query": "I need to understand the data storage architecture"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 15.040381000289926,
          "query_id": "realistic_004",
          "dimension": "casual",
          "query": "database stack"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 15.543287998298183,
          "query_id": "realistic_004",
          "dimension": "contextual",
          "query": "For capacity planning, what databases does CloudFlow rely on?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            }
          ],
          "latency_ms": 15.229823999106884,
          "query_id": "realistic_004",
          "dimension": "negation",
          "query": "Is CloudFlow using MySQL or something else?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            }
          ],
          "latency_ms": 15.370926998002687,
          "query_id": "realistic_005",
          "dimension": "original",
          "query": "What is the Kubernetes namespace for production deployment?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            }
          ],
          "latency_ms": 15.242999001202406,
          "query_id": "realistic_005",
          "dimension": "synonym",
          "query": "Which namespace is used for CloudFlow production?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [],
          "missed_facts": [
            "cloudflow-prod"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            }
          ],
          "latency_ms": 14.920538000296801,
          "query_id": "realistic_005",
          "dimension": "problem",
          "query": "I can't find the production pods"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            }
          ],
          "latency_ms": 14.942398000130197,
          "query_id": "realistic_005",
          "dimension": "casual",
          "query": "prod namespace"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            }
          ],
          "latency_ms": 15.126502003113274,
          "query_id": "realistic_005",
          "dimension": "contextual",
          "query": "I need to deploy to production, what namespace should I use?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            }
          ],
          "latency_ms": 14.937058997020358,
          "query_id": "realistic_005",
          "dimension": "negation",
          "query": "Why can't I see resources in the default namespace?"
        }
      ]
    }
  ]
}