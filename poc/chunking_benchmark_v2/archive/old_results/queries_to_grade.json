{
  "total_queries": 15,
  "queries": [
    {
      "id": "mh_002",
      "query": "If I'm hitting connection pool exhaustion, should I use PgBouncer or add read replicas?",
      "expected_answer": "Both are valid. PgBouncer for connection pooling (max_db_connections=100, pool_mode=transaction). Read replicas for read-heavy workloads. Troubleshooting guide recommends PgBouncer first.",
      "retrieved_chunks": "1. [doc_id: deployment_guide, chunk_id: deployment_guide_mdsem_9, score: 1.000]\n   > #### Pods in CrashLoopBackOff\n   > \n   > **Symptoms**: Pods continuously restart\n   > **Diagnosis**:\n   > ```bash\n   > kubectl logs -n cloudflow-prod <pod-name> --previous\n   > kubectl describe pod -n cloudflow-prod <pod-name>\n   > ```\n   > \n   > **Common Causes**:\n   > - Database connection failure\n   > - Invalid environment variables\n   > - Insufficient resources\n   > \n   > #### High Memory Usage\n   > \n   > **Symptoms**: Pods being OOMKilled\n   > **Diagnosis**:\n   > ```bash\n   > kubectl top pods -n cloudflow-prod\n   > ```\n   > \n   > **Resolution**:\n   > - Increase memory limits in deployment\n   > - Check for me...\n\n2. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_10, score: 0.500]\n   > ## Database Architecture\n   > \n   > \n   > \n   > ### PostgreSQL Primary Database\n   > \n   > **Cluster Configuration**:\n   > - Primary-replica setup with 1 primary + 2 read replicas\n   > - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM)\n   > - Storage: 2TB gp3 SSD with 12,000 IOPS\n   > - Multi-AZ deployment for high availability\n   > - Automated backups: Daily snapshots, 30-day retention\n   > - Point-in-time recovery: 5-minute granularity\n   > \n   > **Database Schema Design**:\n   > \n   > ```\n   > Core Tables:\n   > - users (5M rows): User accounts and profiles\n   > - workflows (2M rows): Wo...\n\n3. [doc_id: deployment_guide, chunk_id: deployment_guide_mdsem_4, score: 0.333]\n   > # postgres-values.yaml\n   > \n   > global:\n   >   postgresql:\n   >     auth:\n   >       username: cloudflow\n   >       database: cloudflow\n   >       existingSecret: postgres-credentials\n   > \n   > image:\n   >   tag: \"14.10.0\"\n   > \n   > primary:\n   >   resources:\n   >     limits:\n   >       cpu: 4000m\n   >       memory: 8Gi\n   >     requests:\n   >       cpu: 2000m\n   >       memory: 4Gi\n   >   \n   >   persistence:\n   >     enabled: true\n   >     size: 100Gi\n   >     storageClass: gp3\n   >   \n   >   extendedConfiguration: |\n   >     max_connections = 100\n   >     shared_buffers = 2GB\n   >     effective_cache_size = 6GB\n   >     maintenance_wor...\n\n4. [doc_id: user_guide, chunk_id: user_guide_mdsem_7, score: 0.250]\n   > ### Database Queries\n   > \n   > Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis):\n   > \n   > **SQL Databases (PostgreSQL, MySQL):**\n   > ```yaml\n   > - id: get_orders\n   >   action: database_query\n   >   config:\n   >     connection: \"{{secrets.DB_CONNECTION_STRING}}\"\n   >     query: |\n   >       SELECT * FROM orders \n   >       WHERE customer_id = $1 \n   >       AND status = $2\n   >       ORDER BY created_at DESC\n   >       LIMIT 10\n   >     parameters:\n   >       - \"{{trigger.customer_id}}\"\n   >       - \"pending\"\n   > ```\n   > \n   > **MongoDB:**\n   > ```yaml\n   > - id: find_do...\n\n5. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_18, score: 0.200]\n   > ### Throughput Capacity\n   > \n   > **API Gateway**:\n   > - Sustained: 10,000 requests per second\n   > - Peak: 25,000 requests per second (5-minute burst)\n   > - Rate limiting: 1,000 requests per minute per API key\n   > \n   > **Workflow Engine**:\n   > - Concurrent executions: 8,000 workflows (across 16 pods)\n   > - Execution start rate: 500 per second\n   > - Completion rate: 450 per second (average 2-second execution time)\n   > \n   > **Database**:\n   > - Read throughput: 50,000 queries per second (across replicas)\n   > - Write throughput: 15,000 transactions per se...",
      "baseline_score": 5,
      "section_text": "mh_002\n**Type**: multi-hop\n**Root Causes**: VOCABULARY_MISMATCH, EMBEDDING_BLIND\n\n**Query**: If I'm hitting connection pool exhaustion, should I use PgBouncer or add read replicas?\n\n**Expected Answer**: Both are valid. PgBouncer for connection pooling (max_db_connections=100, pool_mode=transaction). Read replicas for read-heavy workloads. Troubleshooting guide recommends PgBouncer first.\n\n**Retrieved Chunks**:\n1. [doc_id: deployment_guide, chunk_id: deployment_guide_mdsem_9, score: 1.000]\n   > #### Pods in CrashLoopBackOff\n   > \n   > **Symptoms**: Pods continuously restart\n   > **Diagnosis**:\n   > ```bash\n   > kubectl logs -n cloudflow-prod <pod-name> --previous\n   > kubectl describe pod -n cloudflow-prod <pod-name>\n   > ```\n   > \n   > **Common Causes**:\n   > - Database connection failure\n   > - Invalid environment variables\n   > - Insufficient resources\n   > \n   > #### High Memory Usage\n   > \n   > **Symptoms**: Pods being OOMKilled\n   > **Diagnosis**:\n   > ```bash\n   > kubectl top pods -n cloudflow-prod\n   > ```\n   > \n   > **Resolution**:\n   > - Increase memory limits in deployment\n   > - Check for me...\n\n2. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_10, score: 0.500]\n   > ## Database Architecture\n   > \n   > \n   > \n   > ### PostgreSQL Primary Database\n   > \n   > **Cluster Configuration**:\n   > - Primary-replica setup with 1 primary + 2 read replicas\n   > - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM)\n   > - Storage: 2TB gp3 SSD with 12,000 IOPS\n   > - Multi-AZ deployment for high availability\n   > - Automated backups: Daily snapshots, 30-day retention\n   > - Point-in-time recovery: 5-minute granularity\n   > \n   > **Database Schema Design**:\n   > \n   > ```\n   > Core Tables:\n   > - users (5M rows): User accounts and profiles\n   > - workflows (2M rows): Wo...\n\n3. [doc_id: deployment_guide, chunk_id: deployment_guide_mdsem_4, score: 0.333]\n   > # postgres-values.yaml\n   > \n   > global:\n   >   postgresql:\n   >     auth:\n   >       username: cloudflow\n   >       database: cloudflow\n   >       existingSecret: postgres-credentials\n   > \n   > image:\n   >   tag: \"14.10.0\"\n   > \n   > primary:\n   >   resources:\n   >     limits:\n   >       cpu: 4000m\n   >       memory: 8Gi\n   >     requests:\n   >       cpu: 2000m\n   >       memory: 4Gi\n   >   \n   >   persistence:\n   >     enabled: true\n   >     size: 100Gi\n   >     storageClass: gp3\n   >   \n   >   extendedConfiguration: |\n   >     max_connections = 100\n   >     shared_buffers = 2GB\n   >     effective_cache_size = 6GB\n   >     maintenance_wor...\n\n4. [doc_id: user_guide, chunk_id: user_guide_mdsem_7, score: 0.250]\n   > ### Database Queries\n   > \n   > Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis):\n   > \n   > **SQL Databases (PostgreSQL, MySQL):**\n   > ```yaml\n   > - id: get_orders\n   >   action: database_query\n   >   config:\n   >     connection: \"{{secrets.DB_CONNECTION_STRING}}\"\n   >     query: |\n   >       SELECT * FROM orders \n   >       WHERE customer_id = $1 \n   >       AND status = $2\n   >       ORDER BY created_at DESC\n   >       LIMIT 10\n   >     parameters:\n   >       - \"{{trigger.customer_id}}\"\n   >       - \"pending\"\n   > ```\n   > \n   > **MongoDB:**\n   > ```yaml\n   > - id: find_do...\n\n5. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_18, score: 0.200]\n   > ### Throughput Capacity\n   > \n   > **API Gateway**:\n   > - Sustained: 10,000 requests per second\n   > - Peak: 25,000 requests per second (5-minute burst)\n   > - Rate limiting: 1,000 requests per minute per API key\n   > \n   > **Workflow Engine**:\n   > - Concurrent executions: 8,000 workflows (across 16 pods)\n   > - Execution start rate: 500 per second\n   > - Completion rate: 450 per second (average 2-second execution time)\n   > \n   > **Database**:\n   > - Read throughput: 50,000 queries per second (across replicas)\n   > - Write throughput: 15,000 transactions per se...\n\n**Baseline Score**: 5/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n\n"
    },
    {
      "id": "mh_004",
      "query": "How do the HPA scaling parameters relate to the API Gateway resource requirements?",
      "expected_answer": "HPA: minReplicas=3, maxReplicas=10, targetCPU=70%. API Gateway: 2 vCPU, 4GB RAM per pod. Scales when CPU exceeds 70% of 2 vCPU.",
      "retrieved_chunks": "1. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_8, score: 1.000]\n   > ## Data Flow Architecture\n   > \n   > \n   > \n   > ### Synchronous Request Flow\n   > \n   > Client requests follow a synchronous path through the API Gateway to backend services:\n   > \n   > ```\n   > 1. Client \u2192 Load Balancer (TLS termination)\n   > 2. Load Balancer \u2192 API Gateway (HTTP/2)\n   > 3. API Gateway \u2192 Auth Service (JWT validation via gRPC)\n   > 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC)\n   > 5. Target Service \u2192 Database/Cache (data retrieval)\n   > 6. Response propagates back through the chain\n   > ```\n   > \n   > **Timeout Configuration**:\n   > - Client \u2192 Load Balancer: 6...\n\n2. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_22, score: 0.500]\n   > ### High Availability Architecture\n   > \n   > **Multi-AZ Deployment**:\n   > ```\n   > Region: us-east-1\n   > \n   > AZ-1a:                   AZ-1b:                   AZ-1c:\n   > - API Gateway (4)        - API Gateway (4)        - API Gateway (4)\n   > - Workflow Engine (6)    - Workflow Engine (5)    - Workflow Engine (5)\n   > - Auth Service (3)       - Auth Service (3)       - Auth Service (2)\n   > - PostgreSQL Primary     - PostgreSQL Replica     - PostgreSQL Replica\n   > - Redis Primary (2)      - Redis Replica (2)      - Redis Replica (2)\n   > - Kafka B...\n\n3. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_17, score: 0.333]\n   > ## Performance Characteristics\n   > \n   > \n   > \n   > ### Latency Targets\n   > \n   > **API Operations** (P99 latency):\n   > - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss)\n   > - `POST /workflows`: < 200ms (includes validation and database write)\n   > - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID)\n   > - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page)\n   > \n   > **Workflow Execution** (P99 latency):\n   > - Simple workflow (< 5 steps): < 2 seconds\n   > - Medium workflow (5-15 steps): < 5 seconds\n   > - C...\n\n4. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_1, score: 0.250]\n   > ## Table of Contents\n   > \n   > 1. [High-Level Architecture](#high-level-architecture)\n   > 2. [Microservices Breakdown](#microservices-breakdown)\n   > 3. [Data Flow Architecture](#data-flow-architecture)\n   > 4. [Database Architecture](#database-architecture)\n   > 5. [Message Queue Patterns](#message-queue-patterns)\n   > 6. [Caching Strategy](#caching-strategy)\n   > 7. [Security Architecture](#security-architecture)\n   > 8. [Performance Characteristics](#performance-characteristics)\n   > 9. [Disaster Recovery](#disaster-recovery)\n   > \n   > ---\n   > \n   > ## High...\n\n5. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_3, score: 0.200]\n   > ## Microservices Breakdown\n   > \n   > \n   > \n   > ### API Gateway\n   > \n   > **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.\n   > \n   > **Technology**: Node.js with Express.js framework  \n   > **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU  \n   > **Resource Allocation**: 2 vCPU, 4GB RAM per pod\n   > \n   > **Key Responsibilities**:\n   > - JWT token validation (delegated to Auth Service for initial validation)\n   > - Rate limiting: 1000 requests per minut...",
      "baseline_score": 6,
      "section_text": "mh_004\n**Type**: multi-hop\n**Root Causes**: YAML_BLIND, EMBEDDING_BLIND\n\n**Query**: How do the HPA scaling parameters relate to the API Gateway resource requirements?\n\n**Expected Answer**: HPA: minReplicas=3, maxReplicas=10, targetCPU=70%. API Gateway: 2 vCPU, 4GB RAM per pod. Scales when CPU exceeds 70% of 2 vCPU.\n\n**Retrieved Chunks**:\n1. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_8, score: 1.000]\n   > ## Data Flow Architecture\n   > \n   > \n   > \n   > ### Synchronous Request Flow\n   > \n   > Client requests follow a synchronous path through the API Gateway to backend services:\n   > \n   > ```\n   > 1. Client \u2192 Load Balancer (TLS termination)\n   > 2. Load Balancer \u2192 API Gateway (HTTP/2)\n   > 3. API Gateway \u2192 Auth Service (JWT validation via gRPC)\n   > 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC)\n   > 5. Target Service \u2192 Database/Cache (data retrieval)\n   > 6. Response propagates back through the chain\n   > ```\n   > \n   > **Timeout Configuration**:\n   > - Client \u2192 Load Balancer: 6...\n\n2. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_22, score: 0.500]\n   > ### High Availability Architecture\n   > \n   > **Multi-AZ Deployment**:\n   > ```\n   > Region: us-east-1\n   > \n   > AZ-1a:                   AZ-1b:                   AZ-1c:\n   > - API Gateway (4)        - API Gateway (4)        - API Gateway (4)\n   > - Workflow Engine (6)    - Workflow Engine (5)    - Workflow Engine (5)\n   > - Auth Service (3)       - Auth Service (3)       - Auth Service (2)\n   > - PostgreSQL Primary     - PostgreSQL Replica     - PostgreSQL Replica\n   > - Redis Primary (2)      - Redis Replica (2)      - Redis Replica (2)\n   > - Kafka B...\n\n3. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_17, score: 0.333]\n   > ## Performance Characteristics\n   > \n   > \n   > \n   > ### Latency Targets\n   > \n   > **API Operations** (P99 latency):\n   > - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss)\n   > - `POST /workflows`: < 200ms (includes validation and database write)\n   > - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID)\n   > - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page)\n   > \n   > **Workflow Execution** (P99 latency):\n   > - Simple workflow (< 5 steps): < 2 seconds\n   > - Medium workflow (5-15 steps): < 5 seconds\n   > - C...\n\n4. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_1, score: 0.250]\n   > ## Table of Contents\n   > \n   > 1. [High-Level Architecture](#high-level-architecture)\n   > 2. [Microservices Breakdown](#microservices-breakdown)\n   > 3. [Data Flow Architecture](#data-flow-architecture)\n   > 4. [Database Architecture](#database-architecture)\n   > 5. [Message Queue Patterns](#message-queue-patterns)\n   > 6. [Caching Strategy](#caching-strategy)\n   > 7. [Security Architecture](#security-architecture)\n   > 8. [Performance Characteristics](#performance-characteristics)\n   > 9. [Disaster Recovery](#disaster-recovery)\n   > \n   > ---\n   > \n   > ## High...\n\n5. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_3, score: 0.200]\n   > ## Microservices Breakdown\n   > \n   > \n   > \n   > ### API Gateway\n   > \n   > **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.\n   > \n   > **Technology**: Node.js with Express.js framework  \n   > **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU  \n   > **Resource Allocation**: 2 vCPU, 4GB RAM per pod\n   > \n   > **Key Responsibilities**:\n   > - JWT token validation (delegated to Auth Service for initial validation)\n   > - Rate limiting: 1000 requests per minut...\n\n**Baseline Score**: 6/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n\n"
    },
    {
      "id": "tmp_003",
      "query": "What's the sequence of events when a workflow execution times out?",
      "expected_answer": "Workflow runs up to 3600s. If exceeded, automatically terminated. Error: 'exceeded maximum execution time of 3600 seconds'. Status: TIMEOUT. Can request custom timeout up to 7200s on Enterprise.",
      "retrieved_chunks": "1. [doc_id: user_guide, chunk_id: user_guide_mdsem_13, score: 1.000]\n   > ### Fallback Actions\n   > \n   > Execute alternative actions when the primary action fails:\n   > \n   > ```yaml\n   > - id: primary_payment\n   >   action: http_request\n   >   config:\n   >     url: \"https://primary-payment-gateway.com/charge\"\n   >     method: POST\n   >     body:\n   >       amount: \"{{amount}}\"\n   >   on_error:\n   >     - id: fallback_payment\n   >       action: http_request\n   >       config:\n   >         url: \"https://backup-payment-gateway.com/charge\"\n   >         method: POST\n   >         body:\n   >           amount: \"{{amount}}\"\n   >     - id: notify_admin\n   >       action: email\n   >  ...\n\n2. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_4, score: 0.500]\n   > # Create sub-workflows\n   > \n   > cloudflow workflows create data-pipeline-part1 \\\n   >   --steps \"data_ingestion,data_validation\" \\\n   >   --timeout 1800\n   > \n   > cloudflow workflows create data-pipeline-part2 \\\n   >   --steps \"data_transformation,data_export\" \\\n   >   --timeout 3600 \\\n   >   --trigger workflow_completed \\\n   >   --trigger-workflow data-pipeline-part1\n   > ```\n   > \n   > ### Retry Logic and Exponential Backoff\n   > \n   > CloudFlow implements automatic retry with exponential backoff for transient failures:\n   > - Max retries: 3\n   > - Initial delay: 1 second\n   > -...\n\n3. [doc_id: user_guide, chunk_id: user_guide_mdsem_15, score: 0.333]\n   > ### Execution Limits\n   > \n   > - **Maximum**: 1000 executions per day (per workflow)\n   > - **Rate Limiting**: 100 concurrent executions per workflow\n   > - **Burst Limit**: 10 executions per second\n   > \n   > **What happens when limits are reached:**\n   > - New executions are queued automatically\n   > - Webhook triggers return HTTP 429 (Too Many Requests)\n   > - Scheduled executions are skipped (logged in audit trail)\n   > - Email notifications sent to workflow owner\n   > \n   > **Monitoring Usage:**\n   > View real-time metrics in your workflow dashboard:\n   > - ...\n\n4. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_11, score: 0.250]\n   > #### Step 2: Gather Information (5-15 minutes)\n   > \n   > Create incident document with:\n   > - Incident timestamp and duration\n   > - Affected services and endpoints\n   > - Error rates and user impact\n   > - Recent changes or deployments\n   > - Relevant log excerpts\n   > - Correlation IDs for failed requests\n   > \n   > ```bash\n   > \n   > # Generate incident report\n   > \n   > cloudflow debug incident-report \\\n   >   --start \"2026-01-24T10:30:00Z\" \\\n   >   --end \"2026-01-24T11:00:00Z\" \\\n   >   --output incident-report.md\n   > \n   > # Capture system snapshot\n   > \n   > cloudflow debug snapshot --outp...\n\n5. [doc_id: user_guide, chunk_id: user_guide_mdsem_5, score: 0.200]\n   > ### Triggers\n   > \n   > Triggers determine when your workflow runs. CloudFlow supports several trigger types:\n   > \n   > **Webhook Triggers**\n   > Receive HTTP requests at a unique URL to start your workflow:\n   > - Support GET, POST, PUT, PATCH, DELETE methods\n   > - Automatically parse JSON and form data\n   > - Access headers, query parameters, and body in your workflow\n   > \n   > **Schedule Triggers**\n   > Run workflows on a recurring schedule (see [Scheduling](#scheduling) for details)\n   > \n   > **Event Triggers**\n   > Respond to events from integrated applic...",
      "baseline_score": 7,
      "section_text": "tmp_003\n**Type**: temporal\n**Root Causes**: EMBEDDING_BLIND\n\n**Query**: What's the sequence of events when a workflow execution times out?\n\n**Expected Answer**: Workflow runs up to 3600s. If exceeded, automatically terminated. Error: 'exceeded maximum execution time of 3600 seconds'. Status: TIMEOUT. Can request custom timeout up to 7200s on Enterprise.\n\n**Retrieved Chunks**:\n1. [doc_id: user_guide, chunk_id: user_guide_mdsem_13, score: 1.000]\n   > ### Fallback Actions\n   > \n   > Execute alternative actions when the primary action fails:\n   > \n   > ```yaml\n   > - id: primary_payment\n   >   action: http_request\n   >   config:\n   >     url: \"https://primary-payment-gateway.com/charge\"\n   >     method: POST\n   >     body:\n   >       amount: \"{{amount}}\"\n   >   on_error:\n   >     - id: fallback_payment\n   >       action: http_request\n   >       config:\n   >         url: \"https://backup-payment-gateway.com/charge\"\n   >         method: POST\n   >         body:\n   >           amount: \"{{amount}}\"\n   >     - id: notify_admin\n   >       action: email\n   >  ...\n\n2. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_4, score: 0.500]\n   > # Create sub-workflows\n   > \n   > cloudflow workflows create data-pipeline-part1 \\\n   >   --steps \"data_ingestion,data_validation\" \\\n   >   --timeout 1800\n   > \n   > cloudflow workflows create data-pipeline-part2 \\\n   >   --steps \"data_transformation,data_export\" \\\n   >   --timeout 3600 \\\n   >   --trigger workflow_completed \\\n   >   --trigger-workflow data-pipeline-part1\n   > ```\n   > \n   > ### Retry Logic and Exponential Backoff\n   > \n   > CloudFlow implements automatic retry with exponential backoff for transient failures:\n   > - Max retries: 3\n   > - Initial delay: 1 second\n   > -...\n\n3. [doc_id: user_guide, chunk_id: user_guide_mdsem_15, score: 0.333]\n   > ### Execution Limits\n   > \n   > - **Maximum**: 1000 executions per day (per workflow)\n   > - **Rate Limiting**: 100 concurrent executions per workflow\n   > - **Burst Limit**: 10 executions per second\n   > \n   > **What happens when limits are reached:**\n   > - New executions are queued automatically\n   > - Webhook triggers return HTTP 429 (Too Many Requests)\n   > - Scheduled executions are skipped (logged in audit trail)\n   > - Email notifications sent to workflow owner\n   > \n   > **Monitoring Usage:**\n   > View real-time metrics in your workflow dashboard:\n   > - ...\n\n4. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_11, score: 0.250]\n   > #### Step 2: Gather Information (5-15 minutes)\n   > \n   > Create incident document with:\n   > - Incident timestamp and duration\n   > - Affected services and endpoints\n   > - Error rates and user impact\n   > - Recent changes or deployments\n   > - Relevant log excerpts\n   > - Correlation IDs for failed requests\n   > \n   > ```bash\n   > \n   > # Generate incident report\n   > \n   > cloudflow debug incident-report \\\n   >   --start \"2026-01-24T10:30:00Z\" \\\n   >   --end \"2026-01-24T11:00:00Z\" \\\n   >   --output incident-report.md\n   > \n   > # Capture system snapshot\n   > \n   > cloudflow debug snapshot --outp...\n\n5. [doc_id: user_guide, chunk_id: user_guide_mdsem_5, score: 0.200]\n   > ### Triggers\n   > \n   > Triggers determine when your workflow runs. CloudFlow supports several trigger types:\n   > \n   > **Webhook Triggers**\n   > Receive HTTP requests at a unique URL to start your workflow:\n   > - Support GET, POST, PUT, PATCH, DELETE methods\n   > - Automatically parse JSON and form data\n   > - Access headers, query parameters, and body in your workflow\n   > \n   > **Schedule Triggers**\n   > Run workflows on a recurring schedule (see [Scheduling](#scheduling) for details)\n   > \n   > **Event Triggers**\n   > Respond to events from integrated applic...\n\n**Baseline Score**: 7/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n\n"
    },
    {
      "id": "tmp_004",
      "query": "How long does it take for workflow definition cache changes to propagate?",
      "expected_answer": "Workflow definitions cached in Redis with TTL of 1 hour. Cache invalidated on workflow update or manual flush. Cache hit rate is 94.2%.",
      "retrieved_chunks": "1. [doc_id: user_guide, chunk_id: user_guide_mdsem_17, score: 1.000]\n   > ### 8. Test Thoroughly\n   > \n   > Before activating a workflow:\n   > 1. Use test mode with sample data\n   > 2. Verify all actions execute correctly\n   > 3. Test error handling paths\n   > 4. Review execution logs\n   > 5. Start with a limited scope (e.g., test channel, small dataset)\n   > \n   > ### 9. Document Your Workflows\n   > \n   > Add descriptions to workflows and steps:\n   > \n   > ```yaml\n   > workflow:\n   >   name: \"Daily Sales Report\"\n   >   description: |\n   >     Generates a daily sales report and distributes it to the sales team.\n   >     Runs at 8:00 AM EST Monday-Friday.\n   >  ...\n\n2. [doc_id: user_guide, chunk_id: user_guide_mdsem_10, score: 0.500]\n   > ### Timezone Handling\n   > \n   > All scheduled workflows run in **UTC by default**. To account for your local timezone:\n   > \n   > **Option 1: Convert to UTC**\n   > If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC:\n   > ```\n   > 0 14 * * *  # 9:00 AM EST = 14:00 UTC\n   > ```\n   > \n   > **Option 2: Use Timezone Configuration**\n   > Specify a timezone in your workflow configuration:\n   > ```yaml\n   > schedule:\n   >   cron: \"0 9 * * *\"\n   >   timezone: \"America/New_York\"  # IANA timezone identifier\n   > ```\n   > \n   > **Supported Timezones:**\n   > CloudFlow sup...\n\n3. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_3, score: 0.333]\n   > # Check database slow query log\n   > \n   > kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\\n   >   grep \"slow query\" | \\\n   >   tail -n 50\n   > \n   > # Analyze query patterns\n   > \n   > cloudflow db analyze-queries --min-duration 5000 --limit 20\n   > ```\n   > \n   > **2. Review Query Execution Plans**\n   > \n   > ```sql\n   > -- Connect to CloudFlow database\n   > cloudflow db connect --readonly\n   > \n   > -- Explain slow query\n   > EXPLAIN ANALYZE\n   > SELECT w.*, e.status, e.error_message\n   > FROM workflows w\n   > LEFT JOIN executions e ON w.id = e.workflow_id\n   > WHERE w.workspace_id = 'ws_abc...\n\n4. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_13, score: 0.250]\n   > ### Cache Invalidation Strategies\n   > \n   > **Time-Based Expiration (TTL)**:\n   > - Short-lived data: 5-15 minutes (session tokens, rate limit counters)\n   > - Medium-lived data: 1 hour (workflow definitions, templates)\n   > - Long-lived data: 24 hours (static configuration)\n   > \n   > **Event-Based Invalidation**:\n   > ```\n   > Database Update Event \u2192 Kafka (cache.invalidation topic)\n   >                               \u2502\n   >                               \u25bc\n   >                     All service instances consume event\n   >                               \u2502\n   >     ...\n\n5. [doc_id: user_guide, chunk_id: user_guide_mdsem_4, score: 0.200]\n   > ### YAML Definition\n   > \n   > For advanced users and version control integration, CloudFlow supports YAML-based workflow definitions:\n   > \n   > ```yaml\n   > name: \"Process Customer Orders\"\n   > description: \"Validates and processes new customer orders\"\n   > version: \"1.0\"\n   > \n   > trigger:\n   >   type: webhook\n   >   method: POST\n   >   path: /orders/new\n   > \n   > steps:\n   >   - id: validate_order\n   >     name: \"Validate Order Data\"\n   >     action: javascript\n   >     code: |\n   >       if (!input.order_id || !input.customer_email) {\n   >         throw new Error(\"Missing required field...",
      "baseline_score": 4,
      "section_text": "tmp_004\n**Type**: temporal\n**Root Causes**: EMBEDDING_BLIND\n\n**Query**: How long does it take for workflow definition cache changes to propagate?\n\n**Expected Answer**: Workflow definitions cached in Redis with TTL of 1 hour. Cache invalidated on workflow update or manual flush. Cache hit rate is 94.2%.\n\n**Retrieved Chunks**:\n1. [doc_id: user_guide, chunk_id: user_guide_mdsem_17, score: 1.000]\n   > ### 8. Test Thoroughly\n   > \n   > Before activating a workflow:\n   > 1. Use test mode with sample data\n   > 2. Verify all actions execute correctly\n   > 3. Test error handling paths\n   > 4. Review execution logs\n   > 5. Start with a limited scope (e.g., test channel, small dataset)\n   > \n   > ### 9. Document Your Workflows\n   > \n   > Add descriptions to workflows and steps:\n   > \n   > ```yaml\n   > workflow:\n   >   name: \"Daily Sales Report\"\n   >   description: |\n   >     Generates a daily sales report and distributes it to the sales team.\n   >     Runs at 8:00 AM EST Monday-Friday.\n   >  ...\n\n2. [doc_id: user_guide, chunk_id: user_guide_mdsem_10, score: 0.500]\n   > ### Timezone Handling\n   > \n   > All scheduled workflows run in **UTC by default**. To account for your local timezone:\n   > \n   > **Option 1: Convert to UTC**\n   > If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC:\n   > ```\n   > 0 14 * * *  # 9:00 AM EST = 14:00 UTC\n   > ```\n   > \n   > **Option 2: Use Timezone Configuration**\n   > Specify a timezone in your workflow configuration:\n   > ```yaml\n   > schedule:\n   >   cron: \"0 9 * * *\"\n   >   timezone: \"America/New_York\"  # IANA timezone identifier\n   > ```\n   > \n   > **Supported Timezones:**\n   > CloudFlow sup...\n\n3. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_3, score: 0.333]\n   > # Check database slow query log\n   > \n   > kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\\n   >   grep \"slow query\" | \\\n   >   tail -n 50\n   > \n   > # Analyze query patterns\n   > \n   > cloudflow db analyze-queries --min-duration 5000 --limit 20\n   > ```\n   > \n   > **2. Review Query Execution Plans**\n   > \n   > ```sql\n   > -- Connect to CloudFlow database\n   > cloudflow db connect --readonly\n   > \n   > -- Explain slow query\n   > EXPLAIN ANALYZE\n   > SELECT w.*, e.status, e.error_message\n   > FROM workflows w\n   > LEFT JOIN executions e ON w.id = e.workflow_id\n   > WHERE w.workspace_id = 'ws_abc...\n\n4. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_13, score: 0.250]\n   > ### Cache Invalidation Strategies\n   > \n   > **Time-Based Expiration (TTL)**:\n   > - Short-lived data: 5-15 minutes (session tokens, rate limit counters)\n   > - Medium-lived data: 1 hour (workflow definitions, templates)\n   > - Long-lived data: 24 hours (static configuration)\n   > \n   > **Event-Based Invalidation**:\n   > ```\n   > Database Update Event \u2192 Kafka (cache.invalidation topic)\n   >                               \u2502\n   >                               \u25bc\n   >                     All service instances consume event\n   >                               \u2502\n   >     ...\n\n5. [doc_id: user_guide, chunk_id: user_guide_mdsem_4, score: 0.200]\n   > ### YAML Definition\n   > \n   > For advanced users and version control integration, CloudFlow supports YAML-based workflow definitions:\n   > \n   > ```yaml\n   > name: \"Process Customer Orders\"\n   > description: \"Validates and processes new customer orders\"\n   > version: \"1.0\"\n   > \n   > trigger:\n   >   type: webhook\n   >   method: POST\n   >   path: /orders/new\n   > \n   > steps:\n   >   - id: validate_order\n   >     name: \"Validate Order Data\"\n   >     action: javascript\n   >     code: |\n   >       if (!input.order_id || !input.customer_email) {\n   >         throw new Error(\"Missing required field...\n\n**Baseline Score**: 4/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n\n"
    },
    {
      "id": "tmp_005",
      "query": "What's the timeline for automatic failover when the database primary fails?",
      "expected_answer": "Database primary failure: 30-60 seconds for automatic promotion of replica. Redis failover: <10 seconds. Kafka controller election: <30 seconds.",
      "retrieved_chunks": "1. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_23, score: 1.000]\n   > ### Disaster Recovery Procedures\n   > \n   > **Scenario 1: Single AZ Failure**\n   > - Detection: Health checks fail for entire AZ (< 30 seconds)\n   > - Action: Traffic automatically routed to healthy AZs by ALB\n   > - Recovery time: < 5 minutes (no manual intervention)\n   > - Data loss: None (multi-AZ replication)\n   > \n   > **Scenario 2: Database Primary Failure**\n   > - Detection: Health check fails for primary database (< 30 seconds)\n   > - Action: Automatic promotion of read replica to primary\n   > - Recovery time: 30-60 seconds\n   > - Data loss: Mini...\n\n2. [doc_id: user_guide, chunk_id: user_guide_mdsem_13, score: 0.500]\n   > ### Fallback Actions\n   > \n   > Execute alternative actions when the primary action fails:\n   > \n   > ```yaml\n   > - id: primary_payment\n   >   action: http_request\n   >   config:\n   >     url: \"https://primary-payment-gateway.com/charge\"\n   >     method: POST\n   >     body:\n   >       amount: \"{{amount}}\"\n   >   on_error:\n   >     - id: fallback_payment\n   >       action: http_request\n   >       config:\n   >         url: \"https://backup-payment-gateway.com/charge\"\n   >         method: POST\n   >         body:\n   >           amount: \"{{amount}}\"\n   >     - id: notify_admin\n   >       action: email\n   >  ...\n\n3. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_10, score: 0.333]\n   > ## Database Architecture\n   > \n   > \n   > \n   > ### PostgreSQL Primary Database\n   > \n   > **Cluster Configuration**:\n   > - Primary-replica setup with 1 primary + 2 read replicas\n   > - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM)\n   > - Storage: 2TB gp3 SSD with 12,000 IOPS\n   > - Multi-AZ deployment for high availability\n   > - Automated backups: Daily snapshots, 30-day retention\n   > - Point-in-time recovery: 5-minute granularity\n   > \n   > **Database Schema Design**:\n   > \n   > ```\n   > Core Tables:\n   > - users (5M rows): User accounts and profiles\n   > - workflows (2M rows): Wo...\n\n4. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_24, score: 0.250]\n   > ### Testing & Validation\n   > \n   > **DR Drill Schedule**:\n   > - Monthly: Automated failover test (single AZ failure simulation)\n   > - Quarterly: Full DR region failover (non-production hours)\n   > - Annually: Complete disaster simulation with all stakeholders\n   > \n   > **Last DR Test Results** (Dec 15, 2025):\n   > - Scenario: Full region failover\n   > - Actual RTO: 2 hours 23 minutes (target: 4 hours)\n   > - Actual RPO: 42 minutes (target: 1 hour)\n   > - Issues identified: DNS propagation slower than expected (resolved)\n   > - Success criteria: Met a...\n\n5. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_6, score: 0.200]\n   > ### Scheduler Service\n   > \n   > **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.\n   > \n   > **Technology**: Go with distributed locking via Redis  \n   > **Replicas**: 4 pods (production), active-passive with leader election  \n   > **Resource Allocation**: 2 vCPU, 4GB RAM per pod\n   > \n   > **Key Responsibilities**:\n   > - Parse and validate cron expressions (extended format supporting seconds)\n   > - Maintain schedule registry in PostgreSQL\n   > - Distributed scheduling with leader e...",
      "baseline_score": 7,
      "section_text": "tmp_005\n**Type**: temporal\n**Root Causes**: EMBEDDING_BLIND\n\n**Query**: What's the timeline for automatic failover when the database primary fails?\n\n**Expected Answer**: Database primary failure: 30-60 seconds for automatic promotion of replica. Redis failover: <10 seconds. Kafka controller election: <30 seconds.\n\n**Retrieved Chunks**:\n1. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_23, score: 1.000]\n   > ### Disaster Recovery Procedures\n   > \n   > **Scenario 1: Single AZ Failure**\n   > - Detection: Health checks fail for entire AZ (< 30 seconds)\n   > - Action: Traffic automatically routed to healthy AZs by ALB\n   > - Recovery time: < 5 minutes (no manual intervention)\n   > - Data loss: None (multi-AZ replication)\n   > \n   > **Scenario 2: Database Primary Failure**\n   > - Detection: Health check fails for primary database (< 30 seconds)\n   > - Action: Automatic promotion of read replica to primary\n   > - Recovery time: 30-60 seconds\n   > - Data loss: Mini...\n\n2. [doc_id: user_guide, chunk_id: user_guide_mdsem_13, score: 0.500]\n   > ### Fallback Actions\n   > \n   > Execute alternative actions when the primary action fails:\n   > \n   > ```yaml\n   > - id: primary_payment\n   >   action: http_request\n   >   config:\n   >     url: \"https://primary-payment-gateway.com/charge\"\n   >     method: POST\n   >     body:\n   >       amount: \"{{amount}}\"\n   >   on_error:\n   >     - id: fallback_payment\n   >       action: http_request\n   >       config:\n   >         url: \"https://backup-payment-gateway.com/charge\"\n   >         method: POST\n   >         body:\n   >           amount: \"{{amount}}\"\n   >     - id: notify_admin\n   >       action: email\n   >  ...\n\n3. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_10, score: 0.333]\n   > ## Database Architecture\n   > \n   > \n   > \n   > ### PostgreSQL Primary Database\n   > \n   > **Cluster Configuration**:\n   > - Primary-replica setup with 1 primary + 2 read replicas\n   > - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM)\n   > - Storage: 2TB gp3 SSD with 12,000 IOPS\n   > - Multi-AZ deployment for high availability\n   > - Automated backups: Daily snapshots, 30-day retention\n   > - Point-in-time recovery: 5-minute granularity\n   > \n   > **Database Schema Design**:\n   > \n   > ```\n   > Core Tables:\n   > - users (5M rows): User accounts and profiles\n   > - workflows (2M rows): Wo...\n\n4. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_24, score: 0.250]\n   > ### Testing & Validation\n   > \n   > **DR Drill Schedule**:\n   > - Monthly: Automated failover test (single AZ failure simulation)\n   > - Quarterly: Full DR region failover (non-production hours)\n   > - Annually: Complete disaster simulation with all stakeholders\n   > \n   > **Last DR Test Results** (Dec 15, 2025):\n   > - Scenario: Full region failover\n   > - Actual RTO: 2 hours 23 minutes (target: 4 hours)\n   > - Actual RPO: 42 minutes (target: 1 hour)\n   > - Issues identified: DNS propagation slower than expected (resolved)\n   > - Success criteria: Met a...\n\n5. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_6, score: 0.200]\n   > ### Scheduler Service\n   > \n   > **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.\n   > \n   > **Technology**: Go with distributed locking via Redis  \n   > **Replicas**: 4 pods (production), active-passive with leader election  \n   > **Resource Allocation**: 2 vCPU, 4GB RAM per pod\n   > \n   > **Key Responsibilities**:\n   > - Parse and validate cron expressions (extended format supporting seconds)\n   > - Maintain schedule registry in PostgreSQL\n   > - Distributed scheduling with leader e...\n\n**Baseline Score**: 7/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n\n"
    },
    {
      "id": "cmp_001",
      "query": "What's the difference between PgBouncer connection pooling and direct PostgreSQL connections?",
      "expected_answer": "PgBouncer: pool_mode=transaction, default_pool_size=25, max_db_connections=100. Allows 1000 client connections with only 100 actual DB connections. Direct: limited to max_connections=100.",
      "retrieved_chunks": "1. [doc_id: deployment_guide, chunk_id: deployment_guide_mdsem_4, score: 1.000]\n   > # postgres-values.yaml\n   > \n   > global:\n   >   postgresql:\n   >     auth:\n   >       username: cloudflow\n   >       database: cloudflow\n   >       existingSecret: postgres-credentials\n   > \n   > image:\n   >   tag: \"14.10.0\"\n   > \n   > primary:\n   >   resources:\n   >     limits:\n   >       cpu: 4000m\n   >       memory: 8Gi\n   >     requests:\n   >       cpu: 2000m\n   >       memory: 4Gi\n   >   \n   >   persistence:\n   >     enabled: true\n   >     size: 100Gi\n   >     storageClass: gp3\n   >   \n   >   extendedConfiguration: |\n   >     max_connections = 100\n   >     shared_buffers = 2GB\n   >     effective_cache_size = 6GB\n   >     maintenance_wor...\n\n2. [doc_id: deployment_guide, chunk_id: deployment_guide_mdsem_9, score: 0.500]\n   > #### Pods in CrashLoopBackOff\n   > \n   > **Symptoms**: Pods continuously restart\n   > **Diagnosis**:\n   > ```bash\n   > kubectl logs -n cloudflow-prod <pod-name> --previous\n   > kubectl describe pod -n cloudflow-prod <pod-name>\n   > ```\n   > \n   > **Common Causes**:\n   > - Database connection failure\n   > - Invalid environment variables\n   > - Insufficient resources\n   > \n   > #### High Memory Usage\n   > \n   > **Symptoms**: Pods being OOMKilled\n   > **Diagnosis**:\n   > ```bash\n   > kubectl top pods -n cloudflow-prod\n   > ```\n   > \n   > **Resolution**:\n   > - Increase memory limits in deployment\n   > - Check for me...\n\n3. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_10, score: 0.333]\n   > ## Database Architecture\n   > \n   > \n   > \n   > ### PostgreSQL Primary Database\n   > \n   > **Cluster Configuration**:\n   > - Primary-replica setup with 1 primary + 2 read replicas\n   > - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM)\n   > - Storage: 2TB gp3 SSD with 12,000 IOPS\n   > - Multi-AZ deployment for high availability\n   > - Automated backups: Daily snapshots, 30-day retention\n   > - Point-in-time recovery: 5-minute granularity\n   > \n   > **Database Schema Design**:\n   > \n   > ```\n   > Core Tables:\n   > - users (5M rows): User accounts and profiles\n   > - workflows (2M rows): Wo...\n\n4. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_3, score: 0.250]\n   > # Check database slow query log\n   > \n   > kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\\n   >   grep \"slow query\" | \\\n   >   tail -n 50\n   > \n   > # Analyze query patterns\n   > \n   > cloudflow db analyze-queries --min-duration 5000 --limit 20\n   > ```\n   > \n   > **2. Review Query Execution Plans**\n   > \n   > ```sql\n   > -- Connect to CloudFlow database\n   > cloudflow db connect --readonly\n   > \n   > -- Explain slow query\n   > EXPLAIN ANALYZE\n   > SELECT w.*, e.status, e.error_message\n   > FROM workflows w\n   > LEFT JOIN executions e ON w.id = e.workflow_id\n   > WHERE w.workspace_id = 'ws_abc...\n\n5. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_12, score: 0.200]\n   > ### Kafka Event Streaming\n   > \n   > **Cluster Configuration**:\n   > - 5 broker nodes (distributed across 3 AZs)\n   > - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM)\n   > - Storage: 10TB per broker (gp3 SSD)\n   > - ZooKeeper ensemble: 3 nodes for cluster coordination\n   > - Replication factor: 3 (min in-sync replicas: 2)\n   > \n   > **Topic Architecture**:\n   > \n   > ```\n   > workflow.events (32 partitions):\n   >   - Workflow lifecycle events (created, started, completed, failed)\n   >   - Retention: 7 days\n   >   - Message rate: 5,000/sec peak\n   >   - Consumer groups: ...",
      "baseline_score": 2,
      "section_text": "cmp_001\n**Type**: comparative\n**Root Causes**: VOCABULARY_MISMATCH, EMBEDDING_BLIND\n\n**Query**: What's the difference between PgBouncer connection pooling and direct PostgreSQL connections?\n\n**Expected Answer**: PgBouncer: pool_mode=transaction, default_pool_size=25, max_db_connections=100. Allows 1000 client connections with only 100 actual DB connections. Direct: limited to max_connections=100.\n\n**Retrieved Chunks**:\n1. [doc_id: deployment_guide, chunk_id: deployment_guide_mdsem_4, score: 1.000]\n   > # postgres-values.yaml\n   > \n   > global:\n   >   postgresql:\n   >     auth:\n   >       username: cloudflow\n   >       database: cloudflow\n   >       existingSecret: postgres-credentials\n   > \n   > image:\n   >   tag: \"14.10.0\"\n   > \n   > primary:\n   >   resources:\n   >     limits:\n   >       cpu: 4000m\n   >       memory: 8Gi\n   >     requests:\n   >       cpu: 2000m\n   >       memory: 4Gi\n   >   \n   >   persistence:\n   >     enabled: true\n   >     size: 100Gi\n   >     storageClass: gp3\n   >   \n   >   extendedConfiguration: |\n   >     max_connections = 100\n   >     shared_buffers = 2GB\n   >     effective_cache_size = 6GB\n   >     maintenance_wor...\n\n2. [doc_id: deployment_guide, chunk_id: deployment_guide_mdsem_9, score: 0.500]\n   > #### Pods in CrashLoopBackOff\n   > \n   > **Symptoms**: Pods continuously restart\n   > **Diagnosis**:\n   > ```bash\n   > kubectl logs -n cloudflow-prod <pod-name> --previous\n   > kubectl describe pod -n cloudflow-prod <pod-name>\n   > ```\n   > \n   > **Common Causes**:\n   > - Database connection failure\n   > - Invalid environment variables\n   > - Insufficient resources\n   > \n   > #### High Memory Usage\n   > \n   > **Symptoms**: Pods being OOMKilled\n   > **Diagnosis**:\n   > ```bash\n   > kubectl top pods -n cloudflow-prod\n   > ```\n   > \n   > **Resolution**:\n   > - Increase memory limits in deployment\n   > - Check for me...\n\n3. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_10, score: 0.333]\n   > ## Database Architecture\n   > \n   > \n   > \n   > ### PostgreSQL Primary Database\n   > \n   > **Cluster Configuration**:\n   > - Primary-replica setup with 1 primary + 2 read replicas\n   > - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM)\n   > - Storage: 2TB gp3 SSD with 12,000 IOPS\n   > - Multi-AZ deployment for high availability\n   > - Automated backups: Daily snapshots, 30-day retention\n   > - Point-in-time recovery: 5-minute granularity\n   > \n   > **Database Schema Design**:\n   > \n   > ```\n   > Core Tables:\n   > - users (5M rows): User accounts and profiles\n   > - workflows (2M rows): Wo...\n\n4. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_3, score: 0.250]\n   > # Check database slow query log\n   > \n   > kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\\n   >   grep \"slow query\" | \\\n   >   tail -n 50\n   > \n   > # Analyze query patterns\n   > \n   > cloudflow db analyze-queries --min-duration 5000 --limit 20\n   > ```\n   > \n   > **2. Review Query Execution Plans**\n   > \n   > ```sql\n   > -- Connect to CloudFlow database\n   > cloudflow db connect --readonly\n   > \n   > -- Explain slow query\n   > EXPLAIN ANALYZE\n   > SELECT w.*, e.status, e.error_message\n   > FROM workflows w\n   > LEFT JOIN executions e ON w.id = e.workflow_id\n   > WHERE w.workspace_id = 'ws_abc...\n\n5. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_12, score: 0.200]\n   > ### Kafka Event Streaming\n   > \n   > **Cluster Configuration**:\n   > - 5 broker nodes (distributed across 3 AZs)\n   > - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM)\n   > - Storage: 10TB per broker (gp3 SSD)\n   > - ZooKeeper ensemble: 3 nodes for cluster coordination\n   > - Replication factor: 3 (min in-sync replicas: 2)\n   > \n   > **Topic Architecture**:\n   > \n   > ```\n   > workflow.events (32 partitions):\n   >   - Workflow lifecycle events (created, started, completed, failed)\n   >   - Retention: 7 days\n   >   - Message rate: 5,000/sec peak\n   >   - Consumer groups: ...\n\n**Baseline Score**: 2/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n\n"
    },
    {
      "id": "cmp_002",
      "query": "How do fixed, linear, and exponential backoff strategies differ for retries?",
      "expected_answer": "Fixed: same wait time (1s, 1s, 1s). Linear: increase by fixed amount (1s, 2s, 3s). Exponential: double each time (1s, 2s, 4s). Exponential is recommended.",
      "retrieved_chunks": "1. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_13, score: 1.000]\n   > ### Cache Invalidation Strategies\n   > \n   > **Time-Based Expiration (TTL)**:\n   > - Short-lived data: 5-15 minutes (session tokens, rate limit counters)\n   > - Medium-lived data: 1 hour (workflow definitions, templates)\n   > - Long-lived data: 24 hours (static configuration)\n   > \n   > **Event-Based Invalidation**:\n   > ```\n   > Database Update Event \u2192 Kafka (cache.invalidation topic)\n   >                               \u2502\n   >                               \u25bc\n   >                     All service instances consume event\n   >                               \u2502\n   >     ...\n\n2. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_4, score: 0.500]\n   > # Create sub-workflows\n   > \n   > cloudflow workflows create data-pipeline-part1 \\\n   >   --steps \"data_ingestion,data_validation\" \\\n   >   --timeout 1800\n   > \n   > cloudflow workflows create data-pipeline-part2 \\\n   >   --steps \"data_transformation,data_export\" \\\n   >   --timeout 3600 \\\n   >   --trigger workflow_completed \\\n   >   --trigger-workflow data-pipeline-part1\n   > ```\n   > \n   > ### Retry Logic and Exponential Backoff\n   > \n   > CloudFlow implements automatic retry with exponential backoff for transient failures:\n   > - Max retries: 3\n   > - Initial delay: 1 second\n   > -...\n\n3. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_7, score: 0.333]\n   > #### Handling Rate Limits in Code\n   > \n   > **Python example with retry logic:**\n   > ```python\n   > import time\n   > import requests\n   > \n   > def cloudflow_api_call_with_retry(url, headers, max_retries=3):\n   >     for attempt in range(max_retries):\n   >         response = requests.get(url, headers=headers)\n   >         \n   >         if response.status_code == 429:\n   >             retry_after = int(response.headers.get('Retry-After', 60))\n   >             print(f\"Rate limited. Waiting {retry_after} seconds...\")\n   >             time.sleep(retry_after)\n   >        ...\n\n4. [doc_id: api_reference, chunk_id: api_reference_mdsem_4, score: 0.250]\n   > ## Rate Limiting\n   > \n   > To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints.\n   > \n   > **Default Limits:**\n   > - 100 requests per minute per authenticated user\n   > - 20 requests per minute for unauthenticated requests\n   > - Burst allowance: 150 requests in a 10-second window\n   > \n   > ### Rate Limit Headers\n   > \n   > Every API response includes rate limit information:\n   > \n   > ```\n   > X-RateLimit-Limit: 100\n   > X-RateLimit-Remaining: 87\n   > X-RateLimit-Reset: 1640995200\n   > ```\n   > \n   > When you exceed the rate limit, you'll rec...\n\n5. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_12, score: 0.200]\n   > ### Getting Help\n   > \n   > If this troubleshooting guide doesn't resolve your issue:\n   > \n   > 1. Search the knowledge base: `cloudflow kb search \"your issue\"`\n   > 2. Check community forum for similar issues\n   > 3. Contact support with detailed logs and reproduction steps\n   > 4. For urgent issues, use emergency escalation procedures\n   > \n   > **Remember:** Always capture logs, metrics, and reproduction steps before escalating!\n   > \n   > ---\n   > \n   > *Last updated: January 24, 2026*  \n   > *Document version: 3.2.1*  \n   > *Feedback: docs-feedback@cloudflow.io*",
      "baseline_score": 6,
      "section_text": "cmp_002\n**Type**: comparative\n**Root Causes**: EMBEDDING_BLIND\n\n**Query**: How do fixed, linear, and exponential backoff strategies differ for retries?\n\n**Expected Answer**: Fixed: same wait time (1s, 1s, 1s). Linear: increase by fixed amount (1s, 2s, 3s). Exponential: double each time (1s, 2s, 4s). Exponential is recommended.\n\n**Retrieved Chunks**:\n1. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_13, score: 1.000]\n   > ### Cache Invalidation Strategies\n   > \n   > **Time-Based Expiration (TTL)**:\n   > - Short-lived data: 5-15 minutes (session tokens, rate limit counters)\n   > - Medium-lived data: 1 hour (workflow definitions, templates)\n   > - Long-lived data: 24 hours (static configuration)\n   > \n   > **Event-Based Invalidation**:\n   > ```\n   > Database Update Event \u2192 Kafka (cache.invalidation topic)\n   >                               \u2502\n   >                               \u25bc\n   >                     All service instances consume event\n   >                               \u2502\n   >     ...\n\n2. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_4, score: 0.500]\n   > # Create sub-workflows\n   > \n   > cloudflow workflows create data-pipeline-part1 \\\n   >   --steps \"data_ingestion,data_validation\" \\\n   >   --timeout 1800\n   > \n   > cloudflow workflows create data-pipeline-part2 \\\n   >   --steps \"data_transformation,data_export\" \\\n   >   --timeout 3600 \\\n   >   --trigger workflow_completed \\\n   >   --trigger-workflow data-pipeline-part1\n   > ```\n   > \n   > ### Retry Logic and Exponential Backoff\n   > \n   > CloudFlow implements automatic retry with exponential backoff for transient failures:\n   > - Max retries: 3\n   > - Initial delay: 1 second\n   > -...\n\n3. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_7, score: 0.333]\n   > #### Handling Rate Limits in Code\n   > \n   > **Python example with retry logic:**\n   > ```python\n   > import time\n   > import requests\n   > \n   > def cloudflow_api_call_with_retry(url, headers, max_retries=3):\n   >     for attempt in range(max_retries):\n   >         response = requests.get(url, headers=headers)\n   >         \n   >         if response.status_code == 429:\n   >             retry_after = int(response.headers.get('Retry-After', 60))\n   >             print(f\"Rate limited. Waiting {retry_after} seconds...\")\n   >             time.sleep(retry_after)\n   >        ...\n\n4. [doc_id: api_reference, chunk_id: api_reference_mdsem_4, score: 0.250]\n   > ## Rate Limiting\n   > \n   > To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints.\n   > \n   > **Default Limits:**\n   > - 100 requests per minute per authenticated user\n   > - 20 requests per minute for unauthenticated requests\n   > - Burst allowance: 150 requests in a 10-second window\n   > \n   > ### Rate Limit Headers\n   > \n   > Every API response includes rate limit information:\n   > \n   > ```\n   > X-RateLimit-Limit: 100\n   > X-RateLimit-Remaining: 87\n   > X-RateLimit-Reset: 1640995200\n   > ```\n   > \n   > When you exceed the rate limit, you'll rec...\n\n5. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_12, score: 0.200]\n   > ### Getting Help\n   > \n   > If this troubleshooting guide doesn't resolve your issue:\n   > \n   > 1. Search the knowledge base: `cloudflow kb search \"your issue\"`\n   > 2. Check community forum for similar issues\n   > 3. Contact support with detailed logs and reproduction steps\n   > 4. For urgent issues, use emergency escalation procedures\n   > \n   > **Remember:** Always capture logs, metrics, and reproduction steps before escalating!\n   > \n   > ---\n   > \n   > *Last updated: January 24, 2026*  \n   > *Document version: 3.2.1*  \n   > *Feedback: docs-feedback@cloudflow.io*\n\n**Baseline Score**: 6/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n\n"
    },
    {
      "id": "cmp_003",
      "query": "What's the difference between /health and /ready endpoints?",
      "expected_answer": "/health: liveness check, returns basic status. /ready: readiness check, checks dependencies like database and redis connectivity.",
      "retrieved_chunks": "1. [doc_id: user_guide, chunk_id: user_guide_mdsem_12, score: 1.000]\n   > ## Error Handling\n   > \n   > Robust error handling ensures your workflows are resilient and reliable.\n   > \n   > ### Retry Policies\n   > \n   > Configure automatic retries for failed actions:\n   > \n   > ```yaml\n   > - id: api_call\n   >   action: http_request\n   >   config:\n   >     url: \"https://api.example.com/data\"\n   >   retry:\n   >     max_attempts: 3\n   >     backoff_type: \"exponential\"  # or \"fixed\", \"linear\"\n   >     initial_interval: 1000       # milliseconds\n   >     max_interval: 30000\n   >     multiplier: 2.0\n   >     retry_on:\n   >       - timeout\n   >       - network_error\n   >       - statu...\n\n2. [doc_id: api_reference, chunk_id: api_reference_mdsem_6, score: 0.500]\n   > ### Analytics\n   > \n   > \n   > \n   > #### Get Workflow Metrics\n   > \n   > Retrieve performance metrics and analytics for workflows.\n   > \n   > **Endpoint:** `GET /analytics/workflows/{workflow_id}`\n   > \n   > **Query Parameters:**\n   > - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`)\n   > - `metrics` (optional): Comma-separated list of metrics\n   > \n   > **Available Metrics:**\n   > - `execution_count`: Total number of executions\n   > - `success_rate`: Percentage of successful executions\n   > - `avg_duration`: Average execution duration in milliseconds\n   > - `error_count...\n\n3. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_5, score: 0.333]\n   > ### Workflow Engine\n   > \n   > **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.\n   > \n   > **Technology**: Node.js with TypeScript, Bull queue library  \n   > **Replicas**: 16 pods (production), auto-scaling 12-24  \n   > **Resource Allocation**: 4 vCPU, 8GB RAM per pod\n   > \n   > **Key Responsibilities**:\n   > - Parse and validate workflow definitions (JSON-based DSL)\n   > - Execute workflow steps with state machine pattern\n   > - Handle r...\n\n4. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_10, score: 0.250]\n   > #### SEV-3: Medium (P3)\n   > \n   > - **Definition:** Partial functionality degraded affecting some users\n   > - **Examples:**\n   >   - Intermittent failures for specific workflow types\n   >   - Minor performance issues\n   >   - Non-critical feature unavailable\n   > - **Response Time:** < 4 hours\n   > - **Escalation:** Create ticket, normal business hours support\n   > \n   > #### SEV-4: Low (P4)\n   > \n   > - **Definition:** Minor issues with minimal user impact\n   > - **Examples:**\n   >   - Cosmetic issues\n   >   - Documentation errors\n   >   - Feature requests\n   > - **Response T...\n\n5. [doc_id: user_guide, chunk_id: user_guide_mdsem_6, score: 0.200]\n   > ## Available Actions\n   > \n   > CloudFlow provides a comprehensive library of actions to build powerful automations.\n   > \n   > ### HTTP Requests\n   > \n   > Make HTTP requests to any API endpoint:\n   > \n   > **Configuration:**\n   > - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD\n   > - **URL**: Full endpoint URL (supports variable interpolation)\n   > - **Headers**: Custom headers as key-value pairs\n   > - **Query Parameters**: URL parameters\n   > - **Body**: JSON, form data, or raw text\n   > - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0\n   > \n   > **E...",
      "baseline_score": 5,
      "section_text": "cmp_003\n**Type**: comparative\n**Root Causes**: EMBEDDING_BLIND\n\n**Query**: What's the difference between /health and /ready endpoints?\n\n**Expected Answer**: /health: liveness check, returns basic status. /ready: readiness check, checks dependencies like database and redis connectivity.\n\n**Retrieved Chunks**:\n1. [doc_id: user_guide, chunk_id: user_guide_mdsem_12, score: 1.000]\n   > ## Error Handling\n   > \n   > Robust error handling ensures your workflows are resilient and reliable.\n   > \n   > ### Retry Policies\n   > \n   > Configure automatic retries for failed actions:\n   > \n   > ```yaml\n   > - id: api_call\n   >   action: http_request\n   >   config:\n   >     url: \"https://api.example.com/data\"\n   >   retry:\n   >     max_attempts: 3\n   >     backoff_type: \"exponential\"  # or \"fixed\", \"linear\"\n   >     initial_interval: 1000       # milliseconds\n   >     max_interval: 30000\n   >     multiplier: 2.0\n   >     retry_on:\n   >       - timeout\n   >       - network_error\n   >       - statu...\n\n2. [doc_id: api_reference, chunk_id: api_reference_mdsem_6, score: 0.500]\n   > ### Analytics\n   > \n   > \n   > \n   > #### Get Workflow Metrics\n   > \n   > Retrieve performance metrics and analytics for workflows.\n   > \n   > **Endpoint:** `GET /analytics/workflows/{workflow_id}`\n   > \n   > **Query Parameters:**\n   > - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`)\n   > - `metrics` (optional): Comma-separated list of metrics\n   > \n   > **Available Metrics:**\n   > - `execution_count`: Total number of executions\n   > - `success_rate`: Percentage of successful executions\n   > - `avg_duration`: Average execution duration in milliseconds\n   > - `error_count...\n\n3. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_5, score: 0.333]\n   > ### Workflow Engine\n   > \n   > **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.\n   > \n   > **Technology**: Node.js with TypeScript, Bull queue library  \n   > **Replicas**: 16 pods (production), auto-scaling 12-24  \n   > **Resource Allocation**: 4 vCPU, 8GB RAM per pod\n   > \n   > **Key Responsibilities**:\n   > - Parse and validate workflow definitions (JSON-based DSL)\n   > - Execute workflow steps with state machine pattern\n   > - Handle r...\n\n4. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_10, score: 0.250]\n   > #### SEV-3: Medium (P3)\n   > \n   > - **Definition:** Partial functionality degraded affecting some users\n   > - **Examples:**\n   >   - Intermittent failures for specific workflow types\n   >   - Minor performance issues\n   >   - Non-critical feature unavailable\n   > - **Response Time:** < 4 hours\n   > - **Escalation:** Create ticket, normal business hours support\n   > \n   > #### SEV-4: Low (P4)\n   > \n   > - **Definition:** Minor issues with minimal user impact\n   > - **Examples:**\n   >   - Cosmetic issues\n   >   - Documentation errors\n   >   - Feature requests\n   > - **Response T...\n\n5. [doc_id: user_guide, chunk_id: user_guide_mdsem_6, score: 0.200]\n   > ## Available Actions\n   > \n   > CloudFlow provides a comprehensive library of actions to build powerful automations.\n   > \n   > ### HTTP Requests\n   > \n   > Make HTTP requests to any API endpoint:\n   > \n   > **Configuration:**\n   > - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD\n   > - **URL**: Full endpoint URL (supports variable interpolation)\n   > - **Headers**: Custom headers as key-value pairs\n   > - **Query Parameters**: URL parameters\n   > - **Body**: JSON, form data, or raw text\n   > - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0\n   > \n   > **E...\n\n**Baseline Score**: 5/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n\n"
    },
    {
      "id": "neg_001",
      "query": "What should I NOT do when I'm rate limited?",
      "expected_answer": "Don't keep hammering the API. Instead: check Retry-After header, implement exponential backoff, monitor X-RateLimit-Remaining, cache responses, consider upgrading tier.",
      "retrieved_chunks": "1. [doc_id: api_reference, chunk_id: api_reference_mdsem_4, score: 1.000]\n   > ## Rate Limiting\n   > \n   > To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints.\n   > \n   > **Default Limits:**\n   > - 100 requests per minute per authenticated user\n   > - 20 requests per minute for unauthenticated requests\n   > - Burst allowance: 150 requests in a 10-second window\n   > \n   > ### Rate Limit Headers\n   > \n   > Every API response includes rate limit information:\n   > \n   > ```\n   > X-RateLimit-Limit: 100\n   > X-RateLimit-Remaining: 87\n   > X-RateLimit-Reset: 1640995200\n   > ```\n   > \n   > When you exceed the rate limit, you'll rec...\n\n2. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_7, score: 0.500]\n   > #### Handling Rate Limits in Code\n   > \n   > **Python example with retry logic:**\n   > ```python\n   > import time\n   > import requests\n   > \n   > def cloudflow_api_call_with_retry(url, headers, max_retries=3):\n   >     for attempt in range(max_retries):\n   >         response = requests.get(url, headers=headers)\n   >         \n   >         if response.status_code == 429:\n   >             retry_after = int(response.headers.get('Retry-After', 60))\n   >             print(f\"Rate limited. Waiting {retry_after} seconds...\")\n   >             time.sleep(retry_after)\n   >        ...\n\n3. [doc_id: api_reference, chunk_id: api_reference_mdsem_8, score: 0.333]\n   > ### Error Codes\n   > \n   > CloudFlow returns specific error codes to help you identify and resolve issues:\n   > \n   > - `invalid_parameter`: One or more request parameters are invalid\n   > - `missing_required_field`: Required field is missing from request body\n   > - `authentication_failed`: Invalid API key or token\n   > - `insufficient_permissions`: User lacks required scope or permission\n   > - `resource_not_found`: Requested resource does not exist\n   > - `rate_limit_exceeded`: Too many requests, see rate limiting section\n   > - `workflow_ex...\n\n4. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_6, score: 0.250]\n   > #### Rate Limit Tiers\n   > \n   > CloudFlow enforces the following rate limits per workspace:\n   > \n   > | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows |\n   > |------|-----------------|---------------|----------------------|\n   > | Free | 60 | 1,000 | 5 |\n   > | Standard | 1,000 | 50,000 | 50 |\n   > | Premium | 5,000 | 250,000 | 200 |\n   > | Enterprise | Custom | Custom | Unlimited |\n   > \n   > #### Checking Rate Limit Status\n   > \n   > ```bash\n   > \n   > # Check current rate limit status\n   > \n   > curl -I https://api.cloudflow.io/api/v1/workflows \\\n   >   -H \"Author...\n\n5. [doc_id: user_guide, chunk_id: user_guide_mdsem_13, score: 0.200]\n   > ### Fallback Actions\n   > \n   > Execute alternative actions when the primary action fails:\n   > \n   > ```yaml\n   > - id: primary_payment\n   >   action: http_request\n   >   config:\n   >     url: \"https://primary-payment-gateway.com/charge\"\n   >     method: POST\n   >     body:\n   >       amount: \"{{amount}}\"\n   >   on_error:\n   >     - id: fallback_payment\n   >       action: http_request\n   >       config:\n   >         url: \"https://backup-payment-gateway.com/charge\"\n   >         method: POST\n   >         body:\n   >           amount: \"{{amount}}\"\n   >     - id: notify_admin\n   >       action: email\n   >  ...",
      "baseline_score": 6,
      "section_text": "neg_001\n**Type**: negation\n**Root Causes**: NEGATION_BLIND\n\n**Query**: What should I NOT do when I'm rate limited?\n\n**Expected Answer**: Don't keep hammering the API. Instead: check Retry-After header, implement exponential backoff, monitor X-RateLimit-Remaining, cache responses, consider upgrading tier.\n\n**Retrieved Chunks**:\n1. [doc_id: api_reference, chunk_id: api_reference_mdsem_4, score: 1.000]\n   > ## Rate Limiting\n   > \n   > To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints.\n   > \n   > **Default Limits:**\n   > - 100 requests per minute per authenticated user\n   > - 20 requests per minute for unauthenticated requests\n   > - Burst allowance: 150 requests in a 10-second window\n   > \n   > ### Rate Limit Headers\n   > \n   > Every API response includes rate limit information:\n   > \n   > ```\n   > X-RateLimit-Limit: 100\n   > X-RateLimit-Remaining: 87\n   > X-RateLimit-Reset: 1640995200\n   > ```\n   > \n   > When you exceed the rate limit, you'll rec...\n\n2. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_7, score: 0.500]\n   > #### Handling Rate Limits in Code\n   > \n   > **Python example with retry logic:**\n   > ```python\n   > import time\n   > import requests\n   > \n   > def cloudflow_api_call_with_retry(url, headers, max_retries=3):\n   >     for attempt in range(max_retries):\n   >         response = requests.get(url, headers=headers)\n   >         \n   >         if response.status_code == 429:\n   >             retry_after = int(response.headers.get('Retry-After', 60))\n   >             print(f\"Rate limited. Waiting {retry_after} seconds...\")\n   >             time.sleep(retry_after)\n   >        ...\n\n3. [doc_id: api_reference, chunk_id: api_reference_mdsem_8, score: 0.333]\n   > ### Error Codes\n   > \n   > CloudFlow returns specific error codes to help you identify and resolve issues:\n   > \n   > - `invalid_parameter`: One or more request parameters are invalid\n   > - `missing_required_field`: Required field is missing from request body\n   > - `authentication_failed`: Invalid API key or token\n   > - `insufficient_permissions`: User lacks required scope or permission\n   > - `resource_not_found`: Requested resource does not exist\n   > - `rate_limit_exceeded`: Too many requests, see rate limiting section\n   > - `workflow_ex...\n\n4. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_6, score: 0.250]\n   > #### Rate Limit Tiers\n   > \n   > CloudFlow enforces the following rate limits per workspace:\n   > \n   > | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows |\n   > |------|-----------------|---------------|----------------------|\n   > | Free | 60 | 1,000 | 5 |\n   > | Standard | 1,000 | 50,000 | 50 |\n   > | Premium | 5,000 | 250,000 | 200 |\n   > | Enterprise | Custom | Custom | Unlimited |\n   > \n   > #### Checking Rate Limit Status\n   > \n   > ```bash\n   > \n   > # Check current rate limit status\n   > \n   > curl -I https://api.cloudflow.io/api/v1/workflows \\\n   >   -H \"Author...\n\n5. [doc_id: user_guide, chunk_id: user_guide_mdsem_13, score: 0.200]\n   > ### Fallback Actions\n   > \n   > Execute alternative actions when the primary action fails:\n   > \n   > ```yaml\n   > - id: primary_payment\n   >   action: http_request\n   >   config:\n   >     url: \"https://primary-payment-gateway.com/charge\"\n   >     method: POST\n   >     body:\n   >       amount: \"{{amount}}\"\n   >   on_error:\n   >     - id: fallback_payment\n   >       action: http_request\n   >       config:\n   >         url: \"https://backup-payment-gateway.com/charge\"\n   >         method: POST\n   >         body:\n   >           amount: \"{{amount}}\"\n   >     - id: notify_admin\n   >       action: email\n   >  ...\n\n**Baseline Score**: 6/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n\n"
    },
    {
      "id": "neg_002",
      "query": "Why doesn't HS256 work for JWT token validation in CloudFlow?",
      "expected_answer": "CloudFlow uses RS256 (asymmetric) not HS256 (symmetric). RS256 requires private key for signing, public key for validation. HS256 would fail with algorithm mismatch error.",
      "retrieved_chunks": "1. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_1, score: 1.000]\n   > ## Overview\n   > \n   > This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures.\n   > \n   > ### Quick Diagnostic Checklist\n   > \n   > Before diving into specific issues, perform these initial checks:\n   > \n   > - Verify service health: `cloudflow status --all`\n   > - Check API connectivity: `curl -I https://api.cloudflow.io/health`\n   > - Review recent deployments: `kube...\n\n2. [doc_id: api_reference, chunk_id: api_reference_mdsem_3, score: 0.500]\n   > ### JWT Tokens\n   > \n   > For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims:\n   > \n   > - `iss` (issuer): Your application identifier\n   > - `sub` (subject): User or service account ID\n   > - `aud` (audience): `https://api.cloudflow.io`\n   > - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`)\n   > - `iat` (issued at): Unix timestamp\n   > - `scope`: Space-separated list of requested scopes\n   > \n   > Example JWT header:\n   > \n   > ```python\n   > import jwt\n   > import time...\n\n3. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_15, score: 0.333]\n   > ### Authentication & Authorization\n   > \n   > **JWT Token Validation**:\n   > - Algorithm: RS256 (asymmetric signing)\n   > - Key rotation: Every 30 days with 7-day overlap period\n   > - Public key distribution: JWKS endpoint cached in Redis\n   > - Validation: Signature, expiry, issuer, audience claims\n   > - Token revocation: Blacklist in Redis for compromised tokens\n   > \n   > **Permission Model**:\n   > ```\n   > User \u2192 Roles \u2192 Permissions\n   >      \u2198       \u2197\n   >       Tenants (Multi-tenancy isolation)\n   > ```\n   > \n   > Example permissions:\n   > - `workflow:read` - View workfl...\n\n4. [doc_id: user_guide, chunk_id: user_guide_mdsem_18, score: 0.250]\n   > ### Pattern 5: Error Aggregation and Alerting\n   > \n   > Aggregate errors and send smart alerts:\n   > \n   > ```yaml\n   > name: \"Application Error Monitor\"\n   > schedule:\n   >   cron: \"*/5 * * * *\"\n   >   timezone: \"UTC\"\n   > \n   > steps:\n   >   - id: fetch_recent_errors\n   >     action: database_query\n   >     config:\n   >       query: |\n   >         SELECT error_type, COUNT(*) as count, MAX(created_at) as last_seen\n   >         FROM error_logs\n   >         WHERE created_at > NOW() - INTERVAL '5 minutes'\n   >         AND alerted = false\n   >         GROUP BY error_type\n   >         HAVING COUN...\n\n5. [doc_id: api_reference, chunk_id: api_reference_mdsem_8, score: 0.200]\n   > ### Error Codes\n   > \n   > CloudFlow returns specific error codes to help you identify and resolve issues:\n   > \n   > - `invalid_parameter`: One or more request parameters are invalid\n   > - `missing_required_field`: Required field is missing from request body\n   > - `authentication_failed`: Invalid API key or token\n   > - `insufficient_permissions`: User lacks required scope or permission\n   > - `resource_not_found`: Requested resource does not exist\n   > - `rate_limit_exceeded`: Too many requests, see rate limiting section\n   > - `workflow_ex...",
      "baseline_score": 7,
      "section_text": "neg_002\n**Type**: negation\n**Root Causes**: NEGATION_BLIND\n\n**Query**: Why doesn't HS256 work for JWT token validation in CloudFlow?\n\n**Expected Answer**: CloudFlow uses RS256 (asymmetric) not HS256 (symmetric). RS256 requires private key for signing, public key for validation. HS256 would fail with algorithm mismatch error.\n\n**Retrieved Chunks**:\n1. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_1, score: 1.000]\n   > ## Overview\n   > \n   > This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures.\n   > \n   > ### Quick Diagnostic Checklist\n   > \n   > Before diving into specific issues, perform these initial checks:\n   > \n   > - Verify service health: `cloudflow status --all`\n   > - Check API connectivity: `curl -I https://api.cloudflow.io/health`\n   > - Review recent deployments: `kube...\n\n2. [doc_id: api_reference, chunk_id: api_reference_mdsem_3, score: 0.500]\n   > ### JWT Tokens\n   > \n   > For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims:\n   > \n   > - `iss` (issuer): Your application identifier\n   > - `sub` (subject): User or service account ID\n   > - `aud` (audience): `https://api.cloudflow.io`\n   > - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`)\n   > - `iat` (issued at): Unix timestamp\n   > - `scope`: Space-separated list of requested scopes\n   > \n   > Example JWT header:\n   > \n   > ```python\n   > import jwt\n   > import time...\n\n3. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_15, score: 0.333]\n   > ### Authentication & Authorization\n   > \n   > **JWT Token Validation**:\n   > - Algorithm: RS256 (asymmetric signing)\n   > - Key rotation: Every 30 days with 7-day overlap period\n   > - Public key distribution: JWKS endpoint cached in Redis\n   > - Validation: Signature, expiry, issuer, audience claims\n   > - Token revocation: Blacklist in Redis for compromised tokens\n   > \n   > **Permission Model**:\n   > ```\n   > User \u2192 Roles \u2192 Permissions\n   >      \u2198       \u2197\n   >       Tenants (Multi-tenancy isolation)\n   > ```\n   > \n   > Example permissions:\n   > - `workflow:read` - View workfl...\n\n4. [doc_id: user_guide, chunk_id: user_guide_mdsem_18, score: 0.250]\n   > ### Pattern 5: Error Aggregation and Alerting\n   > \n   > Aggregate errors and send smart alerts:\n   > \n   > ```yaml\n   > name: \"Application Error Monitor\"\n   > schedule:\n   >   cron: \"*/5 * * * *\"\n   >   timezone: \"UTC\"\n   > \n   > steps:\n   >   - id: fetch_recent_errors\n   >     action: database_query\n   >     config:\n   >       query: |\n   >         SELECT error_type, COUNT(*) as count, MAX(created_at) as last_seen\n   >         FROM error_logs\n   >         WHERE created_at > NOW() - INTERVAL '5 minutes'\n   >         AND alerted = false\n   >         GROUP BY error_type\n   >         HAVING COUN...\n\n5. [doc_id: api_reference, chunk_id: api_reference_mdsem_8, score: 0.200]\n   > ### Error Codes\n   > \n   > CloudFlow returns specific error codes to help you identify and resolve issues:\n   > \n   > - `invalid_parameter`: One or more request parameters are invalid\n   > - `missing_required_field`: Required field is missing from request body\n   > - `authentication_failed`: Invalid API key or token\n   > - `insufficient_permissions`: User lacks required scope or permission\n   > - `resource_not_found`: Requested resource does not exist\n   > - `rate_limit_exceeded`: Too many requests, see rate limiting section\n   > - `workflow_ex...\n\n**Baseline Score**: 7/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n\n"
    },
    {
      "id": "neg_003",
      "query": "Why can't I schedule workflows more frequently than every minute?",
      "expected_answer": "Minimum scheduling interval is 1 minute. Expressions evaluating to more frequent executions will be rejected. For near real-time, use webhook or event-based triggers instead.",
      "retrieved_chunks": "1. [doc_id: user_guide, chunk_id: user_guide_mdsem_9, score: 1.000]\n   > ## Scheduling\n   > \n   > CloudFlow supports powerful scheduling options for recurring workflows.\n   > \n   > ### Cron Syntax\n   > \n   > Use standard cron expressions to define schedules:\n   > \n   > ```\n   > *    *    *    *    *\n   > \u252c    \u252c    \u252c    \u252c    \u252c\n   > \u2502    \u2502    \u2502    \u2502    \u2502\n   > \u2502    \u2502    \u2502    \u2502    \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0)\n   > \u2502    \u2502    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12)\n   > \u2502    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31)\n   > \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23)\n   > \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59)\n   > ```\n   > \n   > **Common Cron Patterns:**\n   > \n   > | Pattern | Description |\n   > |--...\n\n2. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_6, score: 0.500]\n   > ### Scheduler Service\n   > \n   > **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.\n   > \n   > **Technology**: Go with distributed locking via Redis  \n   > **Replicas**: 4 pods (production), active-passive with leader election  \n   > **Resource Allocation**: 2 vCPU, 4GB RAM per pod\n   > \n   > **Key Responsibilities**:\n   > - Parse and validate cron expressions (extended format supporting seconds)\n   > - Maintain schedule registry in PostgreSQL\n   > - Distributed scheduling with leader e...\n\n3. [doc_id: user_guide, chunk_id: user_guide_mdsem_11, score: 0.333]\n   > ### Schedule Management\n   > \n   > **Creating a Schedule:**\n   > 1. Open your workflow in the editor\n   > 2. Click the **\"Trigger\"** section\n   > 3. Select **\"Schedule\"** as the trigger type\n   > 4. Enter your cron expression or use the visual schedule builder\n   > 5. Select your timezone\n   > 6. Save and activate\n   > \n   > **Testing Schedules:**\n   > Use the built-in schedule calculator to preview upcoming executions:\n   > ```\n   > Next 5 executions:\n   > 1. 2026-01-24 14:00:00 UTC\n   > 2. 2026-01-25 14:00:00 UTC\n   > 3. 2026-01-26 14:00:00 UTC\n   > 4. 2026-01-27 14:00:00 UTC\n   > ...\n\n4. [doc_id: user_guide, chunk_id: user_guide_mdsem_14, score: 0.250]\n   > ### Steps Per Workflow\n   > \n   > - **Maximum**: 50 steps per workflow\n   > - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.\n   > \n   > ### Execution Timeout\n   > \n   > - **Default**: 3600 seconds (60 minutes)\n   > - **Behavior**: Workflows exceeding this timeout are automatically terminated\n   > - **Custom Timeouts**: Enterprise plans can request custom timeout limits\n   > \n   > **Setting Step-Level Timeouts:**\n   > ```yaml\n   > - id: long_running_task\n   >   actio...\n\n5. [doc_id: user_guide, chunk_id: user_guide_mdsem_10, score: 0.200]\n   > ### Timezone Handling\n   > \n   > All scheduled workflows run in **UTC by default**. To account for your local timezone:\n   > \n   > **Option 1: Convert to UTC**\n   > If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC:\n   > ```\n   > 0 14 * * *  # 9:00 AM EST = 14:00 UTC\n   > ```\n   > \n   > **Option 2: Use Timezone Configuration**\n   > Specify a timezone in your workflow configuration:\n   > ```yaml\n   > schedule:\n   >   cron: \"0 9 * * *\"\n   >   timezone: \"America/New_York\"  # IANA timezone identifier\n   > ```\n   > \n   > **Supported Timezones:**\n   > CloudFlow sup...",
      "baseline_score": 7,
      "section_text": "neg_003\n**Type**: negation\n**Root Causes**: NEGATION_BLIND\n\n**Query**: Why can't I schedule workflows more frequently than every minute?\n\n**Expected Answer**: Minimum scheduling interval is 1 minute. Expressions evaluating to more frequent executions will be rejected. For near real-time, use webhook or event-based triggers instead.\n\n**Retrieved Chunks**:\n1. [doc_id: user_guide, chunk_id: user_guide_mdsem_9, score: 1.000]\n   > ## Scheduling\n   > \n   > CloudFlow supports powerful scheduling options for recurring workflows.\n   > \n   > ### Cron Syntax\n   > \n   > Use standard cron expressions to define schedules:\n   > \n   > ```\n   > *    *    *    *    *\n   > \u252c    \u252c    \u252c    \u252c    \u252c\n   > \u2502    \u2502    \u2502    \u2502    \u2502\n   > \u2502    \u2502    \u2502    \u2502    \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0)\n   > \u2502    \u2502    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12)\n   > \u2502    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31)\n   > \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23)\n   > \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59)\n   > ```\n   > \n   > **Common Cron Patterns:**\n   > \n   > | Pattern | Description |\n   > |--...\n\n2. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_6, score: 0.500]\n   > ### Scheduler Service\n   > \n   > **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.\n   > \n   > **Technology**: Go with distributed locking via Redis  \n   > **Replicas**: 4 pods (production), active-passive with leader election  \n   > **Resource Allocation**: 2 vCPU, 4GB RAM per pod\n   > \n   > **Key Responsibilities**:\n   > - Parse and validate cron expressions (extended format supporting seconds)\n   > - Maintain schedule registry in PostgreSQL\n   > - Distributed scheduling with leader e...\n\n3. [doc_id: user_guide, chunk_id: user_guide_mdsem_11, score: 0.333]\n   > ### Schedule Management\n   > \n   > **Creating a Schedule:**\n   > 1. Open your workflow in the editor\n   > 2. Click the **\"Trigger\"** section\n   > 3. Select **\"Schedule\"** as the trigger type\n   > 4. Enter your cron expression or use the visual schedule builder\n   > 5. Select your timezone\n   > 6. Save and activate\n   > \n   > **Testing Schedules:**\n   > Use the built-in schedule calculator to preview upcoming executions:\n   > ```\n   > Next 5 executions:\n   > 1. 2026-01-24 14:00:00 UTC\n   > 2. 2026-01-25 14:00:00 UTC\n   > 3. 2026-01-26 14:00:00 UTC\n   > 4. 2026-01-27 14:00:00 UTC\n   > ...\n\n4. [doc_id: user_guide, chunk_id: user_guide_mdsem_14, score: 0.250]\n   > ### Steps Per Workflow\n   > \n   > - **Maximum**: 50 steps per workflow\n   > - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.\n   > \n   > ### Execution Timeout\n   > \n   > - **Default**: 3600 seconds (60 minutes)\n   > - **Behavior**: Workflows exceeding this timeout are automatically terminated\n   > - **Custom Timeouts**: Enterprise plans can request custom timeout limits\n   > \n   > **Setting Step-Level Timeouts:**\n   > ```yaml\n   > - id: long_running_task\n   >   actio...\n\n5. [doc_id: user_guide, chunk_id: user_guide_mdsem_10, score: 0.200]\n   > ### Timezone Handling\n   > \n   > All scheduled workflows run in **UTC by default**. To account for your local timezone:\n   > \n   > **Option 1: Convert to UTC**\n   > If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC:\n   > ```\n   > 0 14 * * *  # 9:00 AM EST = 14:00 UTC\n   > ```\n   > \n   > **Option 2: Use Timezone Configuration**\n   > Specify a timezone in your workflow configuration:\n   > ```yaml\n   > schedule:\n   >   cron: \"0 9 * * *\"\n   >   timezone: \"America/New_York\"  # IANA timezone identifier\n   > ```\n   > \n   > **Supported Timezones:**\n   > CloudFlow sup...\n\n**Baseline Score**: 7/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n\n"
    },
    {
      "id": "neg_004",
      "query": "What happens if I don't implement token refresh logic?",
      "expected_answer": "Tokens expire after 3600 seconds (1 hour). Without refresh logic, authentication will fail after expiry. Need to implement refresh using refresh token (valid 7-30 days).",
      "retrieved_chunks": "1. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_1, score: 1.000]\n   > ## Overview\n   > \n   > This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures.\n   > \n   > ### Quick Diagnostic Checklist\n   > \n   > Before diving into specific issues, perform these initial checks:\n   > \n   > - Verify service health: `cloudflow status --all`\n   > - Check API connectivity: `curl -I https://api.cloudflow.io/health`\n   > - Review recent deployments: `kube...\n\n2. [doc_id: api_reference, chunk_id: api_reference_mdsem_3, score: 0.500]\n   > ### JWT Tokens\n   > \n   > For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims:\n   > \n   > - `iss` (issuer): Your application identifier\n   > - `sub` (subject): User or service account ID\n   > - `aud` (audience): `https://api.cloudflow.io`\n   > - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`)\n   > - `iat` (issued at): Unix timestamp\n   > - `scope`: Space-separated list of requested scopes\n   > \n   > Example JWT header:\n   > \n   > ```python\n   > import jwt\n   > import time...\n\n3. [doc_id: api_reference, chunk_id: api_reference_mdsem_8, score: 0.333]\n   > ### Error Codes\n   > \n   > CloudFlow returns specific error codes to help you identify and resolve issues:\n   > \n   > - `invalid_parameter`: One or more request parameters are invalid\n   > - `missing_required_field`: Required field is missing from request body\n   > - `authentication_failed`: Invalid API key or token\n   > - `insufficient_permissions`: User lacks required scope or permission\n   > - `resource_not_found`: Requested resource does not exist\n   > - `rate_limit_exceeded`: Too many requests, see rate limiting section\n   > - `workflow_ex...\n\n4. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_12, score: 0.250]\n   > ### Getting Help\n   > \n   > If this troubleshooting guide doesn't resolve your issue:\n   > \n   > 1. Search the knowledge base: `cloudflow kb search \"your issue\"`\n   > 2. Check community forum for similar issues\n   > 3. Contact support with detailed logs and reproduction steps\n   > 4. For urgent issues, use emergency escalation procedures\n   > \n   > **Remember:** Always capture logs, metrics, and reproduction steps before escalating!\n   > \n   > ---\n   > \n   > *Last updated: January 24, 2026*  \n   > *Document version: 3.2.1*  \n   > *Feedback: docs-feedback@cloudflow.io*\n\n5. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_15, score: 0.200]\n   > ### Authentication & Authorization\n   > \n   > **JWT Token Validation**:\n   > - Algorithm: RS256 (asymmetric signing)\n   > - Key rotation: Every 30 days with 7-day overlap period\n   > - Public key distribution: JWKS endpoint cached in Redis\n   > - Validation: Signature, expiry, issuer, audience claims\n   > - Token revocation: Blacklist in Redis for compromised tokens\n   > \n   > **Permission Model**:\n   > ```\n   > User \u2192 Roles \u2192 Permissions\n   >      \u2198       \u2197\n   >       Tenants (Multi-tenancy isolation)\n   > ```\n   > \n   > Example permissions:\n   > - `workflow:read` - View workfl...",
      "baseline_score": 6,
      "section_text": "neg_004\n**Type**: negation\n**Root Causes**: NEGATION_BLIND\n\n**Query**: What happens if I don't implement token refresh logic?\n\n**Expected Answer**: Tokens expire after 3600 seconds (1 hour). Without refresh logic, authentication will fail after expiry. Need to implement refresh using refresh token (valid 7-30 days).\n\n**Retrieved Chunks**:\n1. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_1, score: 1.000]\n   > ## Overview\n   > \n   > This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures.\n   > \n   > ### Quick Diagnostic Checklist\n   > \n   > Before diving into specific issues, perform these initial checks:\n   > \n   > - Verify service health: `cloudflow status --all`\n   > - Check API connectivity: `curl -I https://api.cloudflow.io/health`\n   > - Review recent deployments: `kube...\n\n2. [doc_id: api_reference, chunk_id: api_reference_mdsem_3, score: 0.500]\n   > ### JWT Tokens\n   > \n   > For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims:\n   > \n   > - `iss` (issuer): Your application identifier\n   > - `sub` (subject): User or service account ID\n   > - `aud` (audience): `https://api.cloudflow.io`\n   > - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`)\n   > - `iat` (issued at): Unix timestamp\n   > - `scope`: Space-separated list of requested scopes\n   > \n   > Example JWT header:\n   > \n   > ```python\n   > import jwt\n   > import time...\n\n3. [doc_id: api_reference, chunk_id: api_reference_mdsem_8, score: 0.333]\n   > ### Error Codes\n   > \n   > CloudFlow returns specific error codes to help you identify and resolve issues:\n   > \n   > - `invalid_parameter`: One or more request parameters are invalid\n   > - `missing_required_field`: Required field is missing from request body\n   > - `authentication_failed`: Invalid API key or token\n   > - `insufficient_permissions`: User lacks required scope or permission\n   > - `resource_not_found`: Requested resource does not exist\n   > - `rate_limit_exceeded`: Too many requests, see rate limiting section\n   > - `workflow_ex...\n\n4. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_12, score: 0.250]\n   > ### Getting Help\n   > \n   > If this troubleshooting guide doesn't resolve your issue:\n   > \n   > 1. Search the knowledge base: `cloudflow kb search \"your issue\"`\n   > 2. Check community forum for similar issues\n   > 3. Contact support with detailed logs and reproduction steps\n   > 4. For urgent issues, use emergency escalation procedures\n   > \n   > **Remember:** Always capture logs, metrics, and reproduction steps before escalating!\n   > \n   > ---\n   > \n   > *Last updated: January 24, 2026*  \n   > *Document version: 3.2.1*  \n   > *Feedback: docs-feedback@cloudflow.io*\n\n5. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_15, score: 0.200]\n   > ### Authentication & Authorization\n   > \n   > **JWT Token Validation**:\n   > - Algorithm: RS256 (asymmetric signing)\n   > - Key rotation: Every 30 days with 7-day overlap period\n   > - Public key distribution: JWKS endpoint cached in Redis\n   > - Validation: Signature, expiry, issuer, audience claims\n   > - Token revocation: Blacklist in Redis for compromised tokens\n   > \n   > **Permission Model**:\n   > ```\n   > User \u2192 Roles \u2192 Permissions\n   >      \u2198       \u2197\n   >       Tenants (Multi-tenancy isolation)\n   > ```\n   > \n   > Example permissions:\n   > - `workflow:read` - View workfl...\n\n**Baseline Score**: 6/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n\n"
    },
    {
      "id": "neg_005",
      "query": "Why shouldn't I hardcode API keys in workflow definitions?",
      "expected_answer": "Security risk - keys could be exposed. Use secrets instead: {{secrets.API_TOKEN}}. Secrets are encrypted at rest. Store in Settings > Secrets.",
      "retrieved_chunks": "1. [doc_id: api_reference, chunk_id: api_reference_mdsem_1, score: 1.000]\n   > ## Authentication\n   > \n   > CloudFlow supports three authentication methods to suit different use cases and security requirements.\n   > \n   > ### API Keys\n   > \n   > API keys provide simple authentication for server-to-server communication. Include your API key in the request header:\n   > \n   > ```bash\n   > curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\\n   >   https://api.cloudflow.io/v2/workflows\n   > ```\n   > \n   > **Security Notes:**\n   > - Never expose API keys in client-side code\n   > - Rotate keys every 90 days\n   > - Use separate keys for development and produc...\n\n2. [doc_id: user_guide, chunk_id: user_guide_mdsem_16, score: 0.500]\n   > ### Data Limits\n   > \n   > - **Maximum request/response size**: 10MB per action\n   > - **Maximum execution payload**: 50MB total\n   > - **Variable value size**: 1MB per variable\n   > \n   > ### Enterprise Plan Limits\n   > \n   > Enterprise customers can request increased limits:\n   > - Up to 100 steps per workflow\n   > - Up to 10,000 executions per day\n   > - Up to 7200 second timeout (2 hours)\n   > - Priority execution queue\n   > - Dedicated capacity allocation\n   > \n   > Contact sales@cloudflow.io for Enterprise pricing and custom limits.\n   > \n   > ## Best Practices\n   > \n   > Follow the...\n\n3. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_15, score: 0.333]\n   > ### Authentication & Authorization\n   > \n   > **JWT Token Validation**:\n   > - Algorithm: RS256 (asymmetric signing)\n   > - Key rotation: Every 30 days with 7-day overlap period\n   > - Public key distribution: JWKS endpoint cached in Redis\n   > - Validation: Signature, expiry, issuer, audience claims\n   > - Token revocation: Blacklist in Redis for compromised tokens\n   > \n   > **Permission Model**:\n   > ```\n   > User \u2192 Roles \u2192 Permissions\n   >      \u2198       \u2197\n   >       Tenants (Multi-tenancy isolation)\n   > ```\n   > \n   > Example permissions:\n   > - `workflow:read` - View workfl...\n\n4. [doc_id: api_reference, chunk_id: api_reference_mdsem_8, score: 0.250]\n   > ### Error Codes\n   > \n   > CloudFlow returns specific error codes to help you identify and resolve issues:\n   > \n   > - `invalid_parameter`: One or more request parameters are invalid\n   > - `missing_required_field`: Required field is missing from request body\n   > - `authentication_failed`: Invalid API key or token\n   > - `insufficient_permissions`: User lacks required scope or permission\n   > - `resource_not_found`: Requested resource does not exist\n   > - `rate_limit_exceeded`: Too many requests, see rate limiting section\n   > - `workflow_ex...\n\n5. [doc_id: api_reference, chunk_id: api_reference_mdsem_0, score: 0.200]\n   > # CloudFlow API Reference\n   > \n   > Version 2.1.0 | Last Updated: January 2026\n   > \n   > ## Overview\n   > \n   > The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices.\n   > \n   > **Base URL:** `https://api.cloudflow.io/v2`\n   > \n   > **API Status:** https://status.cloudflow.io",
      "baseline_score": 5,
      "section_text": "neg_005\n**Type**: negation\n**Root Causes**: NEGATION_BLIND\n\n**Query**: Why shouldn't I hardcode API keys in workflow definitions?\n\n**Expected Answer**: Security risk - keys could be exposed. Use secrets instead: {{secrets.API_TOKEN}}. Secrets are encrypted at rest. Store in Settings > Secrets.\n\n**Retrieved Chunks**:\n1. [doc_id: api_reference, chunk_id: api_reference_mdsem_1, score: 1.000]\n   > ## Authentication\n   > \n   > CloudFlow supports three authentication methods to suit different use cases and security requirements.\n   > \n   > ### API Keys\n   > \n   > API keys provide simple authentication for server-to-server communication. Include your API key in the request header:\n   > \n   > ```bash\n   > curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\\n   >   https://api.cloudflow.io/v2/workflows\n   > ```\n   > \n   > **Security Notes:**\n   > - Never expose API keys in client-side code\n   > - Rotate keys every 90 days\n   > - Use separate keys for development and produc...\n\n2. [doc_id: user_guide, chunk_id: user_guide_mdsem_16, score: 0.500]\n   > ### Data Limits\n   > \n   > - **Maximum request/response size**: 10MB per action\n   > - **Maximum execution payload**: 50MB total\n   > - **Variable value size**: 1MB per variable\n   > \n   > ### Enterprise Plan Limits\n   > \n   > Enterprise customers can request increased limits:\n   > - Up to 100 steps per workflow\n   > - Up to 10,000 executions per day\n   > - Up to 7200 second timeout (2 hours)\n   > - Priority execution queue\n   > - Dedicated capacity allocation\n   > \n   > Contact sales@cloudflow.io for Enterprise pricing and custom limits.\n   > \n   > ## Best Practices\n   > \n   > Follow the...\n\n3. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_15, score: 0.333]\n   > ### Authentication & Authorization\n   > \n   > **JWT Token Validation**:\n   > - Algorithm: RS256 (asymmetric signing)\n   > - Key rotation: Every 30 days with 7-day overlap period\n   > - Public key distribution: JWKS endpoint cached in Redis\n   > - Validation: Signature, expiry, issuer, audience claims\n   > - Token revocation: Blacklist in Redis for compromised tokens\n   > \n   > **Permission Model**:\n   > ```\n   > User \u2192 Roles \u2192 Permissions\n   >      \u2198       \u2197\n   >       Tenants (Multi-tenancy isolation)\n   > ```\n   > \n   > Example permissions:\n   > - `workflow:read` - View workfl...\n\n4. [doc_id: api_reference, chunk_id: api_reference_mdsem_8, score: 0.250]\n   > ### Error Codes\n   > \n   > CloudFlow returns specific error codes to help you identify and resolve issues:\n   > \n   > - `invalid_parameter`: One or more request parameters are invalid\n   > - `missing_required_field`: Required field is missing from request body\n   > - `authentication_failed`: Invalid API key or token\n   > - `insufficient_permissions`: User lacks required scope or permission\n   > - `resource_not_found`: Requested resource does not exist\n   > - `rate_limit_exceeded`: Too many requests, see rate limiting section\n   > - `workflow_ex...\n\n5. [doc_id: api_reference, chunk_id: api_reference_mdsem_0, score: 0.200]\n   > # CloudFlow API Reference\n   > \n   > Version 2.1.0 | Last Updated: January 2026\n   > \n   > ## Overview\n   > \n   > The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices.\n   > \n   > **Base URL:** `https://api.cloudflow.io/v2`\n   > \n   > **API Status:** https://status.cloudflow.io\n\n**Baseline Score**: 5/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n\n"
    },
    {
      "id": "imp_001",
      "query": "Best practice for handling long-running data processing that might exceed time limits",
      "expected_answer": "Workflow timeout is 3600s. Solutions: split into smaller workflows, enable checkpointing (every 300s), use parallel workers, request custom timeout (up to 7200s on Enterprise).",
      "retrieved_chunks": "1. [doc_id: user_guide, chunk_id: user_guide_mdsem_16, score: 1.000]\n   > ### Data Limits\n   > \n   > - **Maximum request/response size**: 10MB per action\n   > - **Maximum execution payload**: 50MB total\n   > - **Variable value size**: 1MB per variable\n   > \n   > ### Enterprise Plan Limits\n   > \n   > Enterprise customers can request increased limits:\n   > - Up to 100 steps per workflow\n   > - Up to 10,000 executions per day\n   > - Up to 7200 second timeout (2 hours)\n   > - Priority execution queue\n   > - Dedicated capacity allocation\n   > \n   > Contact sales@cloudflow.io for Enterprise pricing and custom limits.\n   > \n   > ## Best Practices\n   > \n   > Follow the...\n\n2. [doc_id: user_guide, chunk_id: user_guide_mdsem_12, score: 0.500]\n   > ## Error Handling\n   > \n   > Robust error handling ensures your workflows are resilient and reliable.\n   > \n   > ### Retry Policies\n   > \n   > Configure automatic retries for failed actions:\n   > \n   > ```yaml\n   > - id: api_call\n   >   action: http_request\n   >   config:\n   >     url: \"https://api.example.com/data\"\n   >   retry:\n   >     max_attempts: 3\n   >     backoff_type: \"exponential\"  # or \"fixed\", \"linear\"\n   >     initial_interval: 1000       # milliseconds\n   >     max_interval: 30000\n   >     multiplier: 2.0\n   >     retry_on:\n   >       - timeout\n   >       - network_error\n   >       - statu...\n\n3. [doc_id: api_reference, chunk_id: api_reference_mdsem_4, score: 0.333]\n   > ## Rate Limiting\n   > \n   > To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints.\n   > \n   > **Default Limits:**\n   > - 100 requests per minute per authenticated user\n   > - 20 requests per minute for unauthenticated requests\n   > - Burst allowance: 150 requests in a 10-second window\n   > \n   > ### Rate Limit Headers\n   > \n   > Every API response includes rate limit information:\n   > \n   > ```\n   > X-RateLimit-Limit: 100\n   > X-RateLimit-Remaining: 87\n   > X-RateLimit-Reset: 1640995200\n   > ```\n   > \n   > When you exceed the rate limit, you'll rec...\n\n4. [doc_id: user_guide, chunk_id: user_guide_mdsem_14, score: 0.250]\n   > ### Steps Per Workflow\n   > \n   > - **Maximum**: 50 steps per workflow\n   > - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.\n   > \n   > ### Execution Timeout\n   > \n   > - **Default**: 3600 seconds (60 minutes)\n   > - **Behavior**: Workflows exceeding this timeout are automatically terminated\n   > - **Custom Timeouts**: Enterprise plans can request custom timeout limits\n   > \n   > **Setting Step-Level Timeouts:**\n   > ```yaml\n   > - id: long_running_task\n   >   actio...\n\n5. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_23, score: 0.200]\n   > ### Disaster Recovery Procedures\n   > \n   > **Scenario 1: Single AZ Failure**\n   > - Detection: Health checks fail for entire AZ (< 30 seconds)\n   > - Action: Traffic automatically routed to healthy AZs by ALB\n   > - Recovery time: < 5 minutes (no manual intervention)\n   > - Data loss: None (multi-AZ replication)\n   > \n   > **Scenario 2: Database Primary Failure**\n   > - Detection: Health check fails for primary database (< 30 seconds)\n   > - Action: Automatic promotion of read replica to primary\n   > - Recovery time: 30-60 seconds\n   > - Data loss: Mini...",
      "baseline_score": 6,
      "section_text": "imp_001\n**Type**: implicit\n**Root Causes**: VOCABULARY_MISMATCH, EMBEDDING_BLIND\n\n**Query**: Best practice for handling long-running data processing that might exceed time limits\n\n**Expected Answer**: Workflow timeout is 3600s. Solutions: split into smaller workflows, enable checkpointing (every 300s), use parallel workers, request custom timeout (up to 7200s on Enterprise).\n\n**Retrieved Chunks**:\n1. [doc_id: user_guide, chunk_id: user_guide_mdsem_16, score: 1.000]\n   > ### Data Limits\n   > \n   > - **Maximum request/response size**: 10MB per action\n   > - **Maximum execution payload**: 50MB total\n   > - **Variable value size**: 1MB per variable\n   > \n   > ### Enterprise Plan Limits\n   > \n   > Enterprise customers can request increased limits:\n   > - Up to 100 steps per workflow\n   > - Up to 10,000 executions per day\n   > - Up to 7200 second timeout (2 hours)\n   > - Priority execution queue\n   > - Dedicated capacity allocation\n   > \n   > Contact sales@cloudflow.io for Enterprise pricing and custom limits.\n   > \n   > ## Best Practices\n   > \n   > Follow the...\n\n2. [doc_id: user_guide, chunk_id: user_guide_mdsem_12, score: 0.500]\n   > ## Error Handling\n   > \n   > Robust error handling ensures your workflows are resilient and reliable.\n   > \n   > ### Retry Policies\n   > \n   > Configure automatic retries for failed actions:\n   > \n   > ```yaml\n   > - id: api_call\n   >   action: http_request\n   >   config:\n   >     url: \"https://api.example.com/data\"\n   >   retry:\n   >     max_attempts: 3\n   >     backoff_type: \"exponential\"  # or \"fixed\", \"linear\"\n   >     initial_interval: 1000       # milliseconds\n   >     max_interval: 30000\n   >     multiplier: 2.0\n   >     retry_on:\n   >       - timeout\n   >       - network_error\n   >       - statu...\n\n3. [doc_id: api_reference, chunk_id: api_reference_mdsem_4, score: 0.333]\n   > ## Rate Limiting\n   > \n   > To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints.\n   > \n   > **Default Limits:**\n   > - 100 requests per minute per authenticated user\n   > - 20 requests per minute for unauthenticated requests\n   > - Burst allowance: 150 requests in a 10-second window\n   > \n   > ### Rate Limit Headers\n   > \n   > Every API response includes rate limit information:\n   > \n   > ```\n   > X-RateLimit-Limit: 100\n   > X-RateLimit-Remaining: 87\n   > X-RateLimit-Reset: 1640995200\n   > ```\n   > \n   > When you exceed the rate limit, you'll rec...\n\n4. [doc_id: user_guide, chunk_id: user_guide_mdsem_14, score: 0.250]\n   > ### Steps Per Workflow\n   > \n   > - **Maximum**: 50 steps per workflow\n   > - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.\n   > \n   > ### Execution Timeout\n   > \n   > - **Default**: 3600 seconds (60 minutes)\n   > - **Behavior**: Workflows exceeding this timeout are automatically terminated\n   > - **Custom Timeouts**: Enterprise plans can request custom timeout limits\n   > \n   > **Setting Step-Level Timeouts:**\n   > ```yaml\n   > - id: long_running_task\n   >   actio...\n\n5. [doc_id: architecture_overview, chunk_id: architecture_overview_mdsem_23, score: 0.200]\n   > ### Disaster Recovery Procedures\n   > \n   > **Scenario 1: Single AZ Failure**\n   > - Detection: Health checks fail for entire AZ (< 30 seconds)\n   > - Action: Traffic automatically routed to healthy AZs by ALB\n   > - Recovery time: < 5 minutes (no manual intervention)\n   > - Data loss: None (multi-AZ replication)\n   > \n   > **Scenario 2: Database Primary Failure**\n   > - Detection: Health check fails for primary database (< 30 seconds)\n   > - Action: Automatic promotion of read replica to primary\n   > - Recovery time: 30-60 seconds\n   > - Data loss: Mini...\n\n**Baseline Score**: 6/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n\n"
    },
    {
      "id": "imp_003",
      "query": "How to debug why my API calls are slow",
      "expected_answer": "Check latency breakdown: Auth (18%), DB Query (64%), Business Logic (13%), Serialization (5%). Use cloudflow metrics latency-report. Check slow query log. Review connection pool status.",
      "retrieved_chunks": "1. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_7, score: 1.000]\n   > #### Handling Rate Limits in Code\n   > \n   > **Python example with retry logic:**\n   > ```python\n   > import time\n   > import requests\n   > \n   > def cloudflow_api_call_with_retry(url, headers, max_retries=3):\n   >     for attempt in range(max_retries):\n   >         response = requests.get(url, headers=headers)\n   >         \n   >         if response.status_code == 429:\n   >             retry_after = int(response.headers.get('Retry-After', 60))\n   >             print(f\"Rate limited. Waiting {retry_after} seconds...\")\n   >             time.sleep(retry_after)\n   >        ...\n\n2. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_3, score: 0.500]\n   > # Check database slow query log\n   > \n   > kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\\n   >   grep \"slow query\" | \\\n   >   tail -n 50\n   > \n   > # Analyze query patterns\n   > \n   > cloudflow db analyze-queries --min-duration 5000 --limit 20\n   > ```\n   > \n   > **2. Review Query Execution Plans**\n   > \n   > ```sql\n   > -- Connect to CloudFlow database\n   > cloudflow db connect --readonly\n   > \n   > -- Explain slow query\n   > EXPLAIN ANALYZE\n   > SELECT w.*, e.status, e.error_message\n   > FROM workflows w\n   > LEFT JOIN executions e ON w.id = e.workflow_id\n   > WHERE w.workspace_id = 'ws_abc...\n\n3. [doc_id: api_reference, chunk_id: api_reference_mdsem_4, score: 0.333]\n   > ## Rate Limiting\n   > \n   > To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints.\n   > \n   > **Default Limits:**\n   > - 100 requests per minute per authenticated user\n   > - 20 requests per minute for unauthenticated requests\n   > - Burst allowance: 150 requests in a 10-second window\n   > \n   > ### Rate Limit Headers\n   > \n   > Every API response includes rate limit information:\n   > \n   > ```\n   > X-RateLimit-Limit: 100\n   > X-RateLimit-Remaining: 87\n   > X-RateLimit-Reset: 1640995200\n   > ```\n   > \n   > When you exceed the rate limit, you'll rec...\n\n4. [doc_id: api_reference, chunk_id: api_reference_mdsem_8, score: 0.250]\n   > ### Error Codes\n   > \n   > CloudFlow returns specific error codes to help you identify and resolve issues:\n   > \n   > - `invalid_parameter`: One or more request parameters are invalid\n   > - `missing_required_field`: Required field is missing from request body\n   > - `authentication_failed`: Invalid API key or token\n   > - `insufficient_permissions`: User lacks required scope or permission\n   > - `resource_not_found`: Requested resource does not exist\n   > - `rate_limit_exceeded`: Too many requests, see rate limiting section\n   > - `workflow_ex...\n\n5. [doc_id: user_guide, chunk_id: user_guide_mdsem_16, score: 0.200]\n   > ### Data Limits\n   > \n   > - **Maximum request/response size**: 10MB per action\n   > - **Maximum execution payload**: 50MB total\n   > - **Variable value size**: 1MB per variable\n   > \n   > ### Enterprise Plan Limits\n   > \n   > Enterprise customers can request increased limits:\n   > - Up to 100 steps per workflow\n   > - Up to 10,000 executions per day\n   > - Up to 7200 second timeout (2 hours)\n   > - Priority execution queue\n   > - Dedicated capacity allocation\n   > \n   > Contact sales@cloudflow.io for Enterprise pricing and custom limits.\n   > \n   > ## Best Practices\n   > \n   > Follow the...",
      "baseline_score": 7,
      "section_text": "imp_003\n**Type**: implicit\n**Root Causes**: VOCABULARY_MISMATCH, EMBEDDING_BLIND\n\n**Query**: How to debug why my API calls are slow\n\n**Expected Answer**: Check latency breakdown: Auth (18%), DB Query (64%), Business Logic (13%), Serialization (5%). Use cloudflow metrics latency-report. Check slow query log. Review connection pool status.\n\n**Retrieved Chunks**:\n1. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_7, score: 1.000]\n   > #### Handling Rate Limits in Code\n   > \n   > **Python example with retry logic:**\n   > ```python\n   > import time\n   > import requests\n   > \n   > def cloudflow_api_call_with_retry(url, headers, max_retries=3):\n   >     for attempt in range(max_retries):\n   >         response = requests.get(url, headers=headers)\n   >         \n   >         if response.status_code == 429:\n   >             retry_after = int(response.headers.get('Retry-After', 60))\n   >             print(f\"Rate limited. Waiting {retry_after} seconds...\")\n   >             time.sleep(retry_after)\n   >        ...\n\n2. [doc_id: troubleshooting_guide, chunk_id: troubleshooting_guide_mdsem_3, score: 0.500]\n   > # Check database slow query log\n   > \n   > kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\\n   >   grep \"slow query\" | \\\n   >   tail -n 50\n   > \n   > # Analyze query patterns\n   > \n   > cloudflow db analyze-queries --min-duration 5000 --limit 20\n   > ```\n   > \n   > **2. Review Query Execution Plans**\n   > \n   > ```sql\n   > -- Connect to CloudFlow database\n   > cloudflow db connect --readonly\n   > \n   > -- Explain slow query\n   > EXPLAIN ANALYZE\n   > SELECT w.*, e.status, e.error_message\n   > FROM workflows w\n   > LEFT JOIN executions e ON w.id = e.workflow_id\n   > WHERE w.workspace_id = 'ws_abc...\n\n3. [doc_id: api_reference, chunk_id: api_reference_mdsem_4, score: 0.333]\n   > ## Rate Limiting\n   > \n   > To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints.\n   > \n   > **Default Limits:**\n   > - 100 requests per minute per authenticated user\n   > - 20 requests per minute for unauthenticated requests\n   > - Burst allowance: 150 requests in a 10-second window\n   > \n   > ### Rate Limit Headers\n   > \n   > Every API response includes rate limit information:\n   > \n   > ```\n   > X-RateLimit-Limit: 100\n   > X-RateLimit-Remaining: 87\n   > X-RateLimit-Reset: 1640995200\n   > ```\n   > \n   > When you exceed the rate limit, you'll rec...\n\n4. [doc_id: api_reference, chunk_id: api_reference_mdsem_8, score: 0.250]\n   > ### Error Codes\n   > \n   > CloudFlow returns specific error codes to help you identify and resolve issues:\n   > \n   > - `invalid_parameter`: One or more request parameters are invalid\n   > - `missing_required_field`: Required field is missing from request body\n   > - `authentication_failed`: Invalid API key or token\n   > - `insufficient_permissions`: User lacks required scope or permission\n   > - `resource_not_found`: Requested resource does not exist\n   > - `rate_limit_exceeded`: Too many requests, see rate limiting section\n   > - `workflow_ex...\n\n5. [doc_id: user_guide, chunk_id: user_guide_mdsem_16, score: 0.200]\n   > ### Data Limits\n   > \n   > - **Maximum request/response size**: 10MB per action\n   > - **Maximum execution payload**: 50MB total\n   > - **Variable value size**: 1MB per variable\n   > \n   > ### Enterprise Plan Limits\n   > \n   > Enterprise customers can request increased limits:\n   > - Up to 100 steps per workflow\n   > - Up to 10,000 executions per day\n   > - Up to 7200 second timeout (2 hours)\n   > - Priority execution queue\n   > - Dedicated capacity allocation\n   > \n   > Contact sales@cloudflow.io for Enterprise pricing and custom limits.\n   > \n   > ## Best Practices\n   > \n   > Follow the...\n\n**Baseline Score**: 7/10\n**New Score**: ___/10 (FILL IN)\n**Notes**: _______________\n\n---\n"
    }
  ]
}