{
  "description": "Ground truth queries for chunking benchmark evaluation",
  "version": "1.0",
  "queries": [
    {
      "id": "simple_01",
      "category": "simple_lookup",
      "query": "What database does CloudFlow use?",
      "expected_docs": ["arch_database", "adr_001"],
      "expected_sections": ["Database Design > Overview", "ADR-001 > Decision"],
      "ground_truth_answer": "CloudFlow uses PostgreSQL 15 as the primary database.",
      "key_facts": ["PostgreSQL", "version 15", "AWS RDS"]
    },
    {
      "id": "simple_02",
      "category": "simple_lookup",
      "query": "What is the rate limit for API requests?",
      "expected_docs": ["arch_overview", "arch_api"],
      "expected_sections": ["API Gateway", "Rate Limiting"],
      "ground_truth_answer": "The rate limit is 100 requests/minute per user for standard tier, 1000 for Pro, and custom limits for Enterprise.",
      "key_facts": ["100 requests/minute", "Standard tier", "1000 for Pro"]
    },
    {
      "id": "simple_03",
      "category": "simple_lookup",
      "query": "How long do JWT tokens last before expiring?",
      "expected_docs": ["arch_overview", "adr_003"],
      "expected_sections": ["Authentication", "Token Lifetimes"],
      "ground_truth_answer": "Access tokens expire after 1 hour. Refresh tokens last 7 days.",
      "key_facts": ["1 hour", "refresh tokens", "7 days"]
    },
    {
      "id": "simple_04",
      "category": "simple_lookup",
      "query": "What connection pooler does CloudFlow use?",
      "expected_docs": ["arch_overview", "arch_database"],
      "expected_sections": ["Database Architecture", "Connection Pooling"],
      "ground_truth_answer": "CloudFlow uses PgBouncer for connection pooling with a pool size of 100 connections per service.",
      "key_facts": ["PgBouncer", "100 connections", "transaction mode"]
    },
    {
      "id": "simple_05",
      "category": "simple_lookup",
      "query": "What encryption is used for data at rest?",
      "expected_docs": ["arch_overview", "security_policy"],
      "expected_sections": ["Data Encryption", "Encryption"],
      "ground_truth_answer": "All data is encrypted at rest using AES-256. Data in transit uses TLS 1.3.",
      "key_facts": ["AES-256", "at rest", "TLS 1.3"]
    },
    {
      "id": "section_01",
      "category": "section_retrieval",
      "query": "Explain the workflow engine execution model",
      "expected_docs": ["arch_overview"],
      "expected_sections": ["Workflow Engine > Execution Model"],
      "ground_truth_answer": "Workflows are executed using an event-driven model. Each step produces events that trigger subsequent steps. The engine supports sequential execution, parallel execution, conditional branching, and error handling with retries.",
      "key_facts": ["event-driven", "sequential", "parallel", "conditional branching", "retries"]
    },
    {
      "id": "section_02",
      "category": "section_retrieval",
      "query": "How do I deploy CloudFlow to production?",
      "expected_docs": ["howto_deploy"],
      "expected_sections": ["Deployment Process"],
      "ground_truth_answer": "Deploy by: 1) Building container images with make build-all, 2) Running database migrations with Flyway, 3) Deploying with Helm using helm upgrade --install, 4) Verifying with kubectl get pods and health checks.",
      "key_facts": ["make build-all", "Flyway", "helm upgrade --install", "kubectl get pods"]
    },
    {
      "id": "section_03",
      "category": "section_retrieval",
      "query": "What are the RBAC roles in CloudFlow?",
      "expected_docs": ["arch_overview", "security_policy"],
      "expected_sections": ["Authorization", "Access Control"],
      "ground_truth_answer": "CloudFlow has four RBAC roles: Owner (full access + billing), Admin (user management + all workflows), Editor (create/edit workflows), and Viewer (read-only access).",
      "key_facts": ["Owner", "Admin", "Editor", "Viewer", "RBAC"]
    },
    {
      "id": "section_04",
      "category": "section_retrieval",
      "query": "How do I create a workflow with a webhook trigger?",
      "expected_docs": ["howto_workflows"],
      "expected_sections": ["Trigger Types > Webhook Trigger"],
      "ground_truth_answer": "Create a webhook trigger by specifying type: webhook in the trigger section, with a path like /process-order and optionally a method like POST.",
      "key_facts": ["type: webhook", "path", "method: POST"]
    },
    {
      "id": "section_05",
      "category": "section_retrieval",
      "query": "What is the incident severity classification?",
      "expected_docs": ["runbook_incidents"],
      "expected_sections": ["Severity Levels"],
      "ground_truth_answer": "P1 (Critical): service down, data loss, security breach - immediate response, 1hr resolution. P2 (High): major feature unavailable, >10% users affected - 15min response, 4hr resolution. P3 (Medium): minor issues with workaround - 1hr response, 24hr resolution.",
      "key_facts": ["P1", "P2", "P3", "Critical", "High", "Medium"]
    },
    {
      "id": "cross_01",
      "category": "cross_document",
      "query": "How is authentication implemented across the system?",
      "expected_docs": ["arch_overview", "adr_003", "arch_api", "security_policy"],
      "expected_sections": ["Authentication", "API Authentication Strategy", "Security"],
      "ground_truth_answer": "CloudFlow uses OAuth 2.0 with JWT tokens. Tokens contain user_id, email, role, and expire after 1 hour. API keys are also supported for machine-to-machine auth. Passwords are hashed with bcrypt (cost factor 12), and MFA is required for admin accounts.",
      "key_facts": ["OAuth 2.0", "JWT", "API keys", "bcrypt", "MFA"]
    },
    {
      "id": "cross_02",
      "category": "cross_document",
      "query": "What monitoring and observability tools does CloudFlow use?",
      "expected_docs": ["arch_overview", "howto_deploy", "runbook_incidents"],
      "expected_sections": ["Infrastructure", "Monitoring", "Diagnosis"],
      "ground_truth_answer": "CloudFlow uses Prometheus + Grafana for monitoring, with metrics exposed at /metrics. Logs are sent to CloudWatch, errors to Sentry. Kubernetes monitoring uses kubectl commands for diagnosis.",
      "key_facts": ["Prometheus", "Grafana", "CloudWatch", "Sentry", "/metrics"]
    },
    {
      "id": "cross_03",
      "category": "cross_document",
      "query": "What are the database tables and their relationships?",
      "expected_docs": ["arch_database"],
      "expected_sections": ["Schema Design > Core Tables"],
      "ground_truth_answer": "Core tables: users (id, email, password_hash, name, role), workflows (id, user_id FK, name, description, definition JSONB, status), executions (id, workflow_id FK, status, started_at, completed_at, result JSONB, error).",
      "key_facts": ["users", "workflows", "executions", "user_id FK", "workflow_id FK", "JSONB"]
    },
    {
      "id": "cross_04",
      "category": "cross_document",
      "query": "What are all the API endpoints for workflows?",
      "expected_docs": ["arch_api", "api_reference"],
      "expected_sections": ["Endpoints > Workflows", "REST API Design"],
      "ground_truth_answer": "Workflow endpoints: GET /workflows (list), POST /workflows (create), GET /workflows/{id} (get), PUT /workflows/{id} (update), DELETE /workflows/{id} (delete), POST /workflows/{id}/execute (execute).",
      "key_facts": ["GET /workflows", "POST /workflows", "PUT /workflows/{id}", "DELETE /workflows/{id}", "POST /workflows/{id}/execute"]
    },
    {
      "id": "comparison_01",
      "category": "comparison",
      "query": "What database options were considered and why was PostgreSQL chosen?",
      "expected_docs": ["adr_001"],
      "expected_sections": ["Options Considered", "Decision", "Rationale"],
      "ground_truth_answer": "Three options were considered: PostgreSQL (chosen for JSONB support, team expertise, mature ecosystem), MongoDB (rejected due to weaker consistency, team training needed), CockroachDB (rejected due to higher complexity and cost). PostgreSQL was chosen because the team has expertise, it has proven reliability, and JSONB covers flexible workflow definitions.",
      "key_facts": ["PostgreSQL", "MongoDB", "CockroachDB", "JSONB", "team expertise", "consistency"]
    },
    {
      "id": "comparison_02",
      "category": "comparison",
      "query": "Compare the different trigger types available for workflows",
      "expected_docs": ["howto_workflows"],
      "expected_sections": ["Trigger Types"],
      "ground_truth_answer": "Four trigger types: Manual (execute via API/CLI), Webhook (HTTP endpoint that starts workflow), Schedule (cron-based, e.g., '0 9 * * *' for daily at 9 AM), Event (react to system events like S3 object.created).",
      "key_facts": ["manual", "webhook", "schedule", "event", "cron"]
    },
    {
      "id": "comparison_03",
      "category": "comparison",
      "query": "What are the pros and cons of Kubernetes vs ECS?",
      "expected_docs": ["adr_002"],
      "expected_sections": ["Options Considered"],
      "ground_truth_answer": "Kubernetes pros: industry standard, rich ecosystem, auto-scaling, strong community. Cons: complex to operate, learning curve. ECS pros: simpler, native AWS integration, lower operational overhead. Cons: AWS lock-in, less flexible, smaller ecosystem.",
      "key_facts": ["Kubernetes", "ECS", "industry standard", "AWS lock-in", "ecosystem"]
    },
    {
      "id": "exhaustive_01",
      "category": "exhaustive",
      "query": "List all environment variables mentioned in the documentation",
      "expected_docs": ["howto_deploy", "howto_quickstart"],
      "expected_sections": ["Environment Variables"],
      "ground_truth_answer": "Environment variables: DATABASE_URL (PostgreSQL connection string), REDIS_URL (Redis connection string), JWT_SECRET (secret for JWT signing), AWS_REGION (AWS region), CLOUDFLOW_API_KEY (API authentication).",
      "key_facts": ["DATABASE_URL", "REDIS_URL", "JWT_SECRET", "AWS_REGION", "CLOUDFLOW_API_KEY"]
    },
    {
      "id": "exhaustive_02",
      "category": "exhaustive",
      "query": "What are all the error HTTP status codes the API can return?",
      "expected_docs": ["arch_api"],
      "expected_sections": ["Error Handling"],
      "ground_truth_answer": "Error status codes: 400 (Bad Request), 401 (Unauthorized), 403 (Forbidden), 404 (Not Found), 429 (Too Many Requests), 500 (Internal Server Error).",
      "key_facts": ["400", "401", "403", "404", "429", "500"]
    },
    {
      "id": "exhaustive_03",
      "category": "exhaustive",
      "query": "List all the data stores used in CloudFlow",
      "expected_docs": ["arch_overview", "arch_database"],
      "expected_sections": ["Data Layer"],
      "ground_truth_answer": "CloudFlow uses: PostgreSQL (transactional data, workflow state), Redis (caching, session storage), S3 (file storage), Elasticsearch (search and analytics).",
      "key_facts": ["PostgreSQL", "Redis", "S3", "Elasticsearch"]
    },
    {
      "id": "decision_01",
      "category": "decision_rationale",
      "query": "Why did CloudFlow choose Kubernetes over other options?",
      "expected_docs": ["adr_002"],
      "expected_sections": ["Rationale", "Decision"],
      "ground_truth_answer": "Kubernetes was chosen because: 1) Industry standard - easy to hire engineers, 2) Rich ecosystem (Helm, Istio, Prometheus), 3) Cloud-agnostic - can migrate to GKE/AKS, 4) Powerful auto-scaling, 5) Strong secret management with external-secrets operator.",
      "key_facts": ["industry standard", "ecosystem", "cloud-agnostic", "auto-scaling", "secret management"]
    },
    {
      "id": "decision_02",
      "category": "decision_rationale",
      "query": "Why was JWT with OAuth 2.0 chosen for authentication?",
      "expected_docs": ["adr_003"],
      "expected_sections": ["Decision", "Rationale"],
      "ground_truth_answer": "JWT with OAuth 2.0 was chosen because it's an industry standard, enables stateless verification, works with SSO providers, and supports refresh tokens. The alternatives (session-based, API keys only) were rejected due to scalability concerns and lack of SSO support.",
      "key_facts": ["industry standard", "stateless verification", "SSO", "refresh tokens"]
    },
    {
      "id": "decision_03",
      "category": "decision_rationale",
      "query": "What are the consequences of choosing PostgreSQL?",
      "expected_docs": ["adr_001"],
      "expected_sections": ["Consequences"],
      "ground_truth_answer": "Consequences include: need for application-level sharding if outgrowing single-node, must set up PgBouncer for connection pooling, need to invest in monitoring and query optimization, and database migrations require careful planning.",
      "key_facts": ["application-level sharding", "PgBouncer", "monitoring", "query optimization", "migrations"]
    },
    {
      "id": "howto_01",
      "category": "how_to",
      "query": "How do I install the CloudFlow CLI?",
      "expected_docs": ["howto_quickstart"],
      "expected_sections": ["Step 1: Install the CLI"],
      "ground_truth_answer": "Install CLI: macOS: brew install cloudflow-cli, Linux: curl -fsSL https://get.cloudflow.io | bash, Windows: choco install cloudflow-cli.",
      "key_facts": ["brew install", "curl -fsSL", "choco install"]
    },
    {
      "id": "howto_02",
      "category": "how_to",
      "query": "How do I rollback a failed deployment?",
      "expected_docs": ["howto_deploy"],
      "expected_sections": ["Rollback Procedure"],
      "ground_truth_answer": "Rollback with: helm rollback cloudflow -n production (to previous version), or helm rollback cloudflow 5 -n production (to specific revision number).",
      "key_facts": ["helm rollback", "-n production", "revision number"]
    },
    {
      "id": "howto_03",
      "category": "how_to",
      "query": "How do I handle errors in workflows?",
      "expected_docs": ["howto_workflows"],
      "expected_sections": ["Error Handling"],
      "ground_truth_answer": "Handle errors using retry configuration (attempts, delay, backoff: exponential) or on_error handlers that can trigger notification steps like type: notify with channel and message.",
      "key_facts": ["retry", "attempts", "backoff: exponential", "on_error", "notify"]
    },
    {
      "id": "howto_04",
      "category": "how_to",
      "query": "How do I set up local development?",
      "expected_docs": ["onboarding"],
      "expected_sections": ["Local Development"],
      "ground_truth_answer": "Set up local dev: 1) Clone repo: git clone git@github.com:cloudflow/cloudflow.git, 2) Install deps: make setup, 3) Start env: docker-compose up -d && make dev, 4) Run tests: make test. Local services run at API localhost:8080, UI localhost:3000.",
      "key_facts": ["git clone", "make setup", "docker-compose up", "make dev", "localhost:8080"]
    },
    {
      "id": "troubleshoot_01",
      "category": "troubleshooting",
      "query": "How do I diagnose API Gateway 502 errors?",
      "expected_docs": ["runbook_incidents"],
      "expected_sections": ["Common Issues > API Gateway 502 Errors"],
      "ground_truth_answer": "Diagnose 502 errors: Check Kong logs with kubectl logs -l app=kong, verify pod health with kubectl get pods and describe pod, check endpoints with kubectl get endpoints. Common causes: unhealthy pods, OOMKilled containers.",
      "key_facts": ["kubectl logs -l app=kong", "kubectl get pods", "kubectl get endpoints", "OOMKilled"]
    },
    {
      "id": "troubleshoot_02",
      "category": "troubleshooting",
      "query": "What to do if database connections are failing?",
      "expected_docs": ["runbook_incidents"],
      "expected_sections": ["Common Issues > Database Connection Errors"],
      "ground_truth_answer": "For DB connection errors: Check PgBouncer stats with SHOW POOLS, check connection count with SELECT count(*) FROM pg_stat_activity, verify PgBouncer is running, look for long-running queries, kill stuck queries if needed.",
      "key_facts": ["SHOW POOLS", "pg_stat_activity", "PgBouncer", "long-running queries"]
    },
    {
      "id": "factual_01",
      "category": "factual",
      "query": "What was discussed in the Q1 2024 planning meeting?",
      "expected_docs": ["meeting_notes_q1"],
      "expected_sections": ["Q1 Priorities"],
      "ground_truth_answer": "Q1 priorities: P0 (must have): workflow versioning by end of February, improved monitoring by end of January. P1 (should have): new workflow editor by end of March, Python SDK v2 by end of February. Also discussed Q4 accomplishments including GraphQL API launch and 50% user growth.",
      "key_facts": ["workflow versioning", "monitoring", "workflow editor", "Python SDK v2", "GraphQL API"]
    }
  ]
}
