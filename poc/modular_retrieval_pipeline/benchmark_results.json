{
  "strategy": "modular-no-llm",
  "benchmark_run_at": "2026-01-29T23:08:53.286768",
  "questions_file": "poc/chunking_benchmark_v2/corpus/needle_questions.json",
  "accuracy": 90.0,
  "hit_at_1_rate": 65.0,
  "hit_at_5_rate": 90.0,
  "mrr": 0.7458333333333333,
  "avg_llm_grade": 7.6,
  "avg_total_score": 7.32,
  "pass_rate_8": 65.0,
  "pass_rate_7": 65.0,
  "pass_rate_6_5": 65.0,
  "avg_latency_ms": 25.174474716186523,
  "peak_memory_mb": 132.78175830841064,
  "results": [
    {
      "question_id": "q_001",
      "question": "My pod keeps getting rejected with a topology affinity error, what's going on?",
      "expected_answer": "This happens with restricted or single-numa-node policies when the Topology Manager cannot find a suitable NUMA node affinity. The pod enters a Terminated state. Use a Deployment or ReplicaSet to trigger redeployment.",
      "needle_found": true,
      "rank": 3,
      "hit_at_1": false,
      "hit_at_5": true,
      "llm_grade": 10,
      "llm_reasoning": "The retrieved chunks contain all key facts from the expected answer. Chunk 3 explicitly states that with the 'restricted' topology management policy, if the affinity is not preferred, the Topology Manager will reject the pod, resulting in a 'Terminated' state with a pod admission failure. It also specifically mentions that the Kubernetes scheduler will NOT attempt to reschedule the pod and recommends using a ReplicaSet or Deployment to trigger redeployment. Chunk 2 explains that TopologyAffinityError occurs when a node doesn't have enough resources or the pod's request is rejected due to Topology Manager policy constraints, which covers the 'restricted or single-numa-node policies' and 'cannot find suitable NUMA node affinity' aspects. The chunks provide complete information to understand why the pod is being rejected and how to resolve it through redeployment.",
      "total_score": 9.5,
      "latency_ms": 29.9,
      "grading_latency_ms": 4635.532379150391
    },
    {
      "question_id": "q_002",
      "question": "How do I force all my containers to run on the same NUMA node in k8s?",
      "expected_answer": "Use the pod scope with single-numa-node policy. Set topologyManagerScope to pod in kubelet config and use --topology-manager-policy=single-numa-node",
      "needle_found": true,
      "rank": 1,
      "hit_at_1": true,
      "hit_at_5": true,
      "llm_grade": 10,
      "llm_reasoning": "The retrieved chunks contain all key facts from the expected answer. Chunk 1 explicitly states to 'set `topologyManagerScope` in the kubelet configuration file to `pod`' (matches 'Set topologyManagerScope to pod in kubelet config'). Chunk 1 also clearly explains 'Using the `pod` scope in tandem with `single-numa-node` Topology Manager policy' and states 'In the case of `single-numa-node` policy, a pod is accepted only if a suitable set of NUMA nodes is present' (matches 'use --topology-manager-policy=single-numa-node'). Chunk 5 reinforces the scope configuration details. The chunks provide complete, actionable information: configure topologyManagerScope to pod in kubelet config file and use single-numa-node policy to force all containers in a pod onto the same NUMA node. All essential elements from the expected answer are present.",
      "total_score": 10.0,
      "latency_ms": 31.5,
      "grading_latency_ms": 5743.6089515686035
    },
    {
      "question_id": "q_003",
      "question": "What's the difference between restricted and best-effort topology policies?",
      "expected_answer": "best-effort admits pods even if preferred NUMA affinity isn't available, while restricted rejects pods that can't get preferred affinity, putting them in Terminated state",
      "needle_found": true,
      "rank": 1,
      "hit_at_1": true,
      "hit_at_5": true,
      "llm_grade": 6,
      "llm_reasoning": "Chunk 1 explains that best-effort 'will store this and admit the pod to the node anyway' even if affinity is not preferred, which aligns with the expected answer that best-effort admits pods even without preferred affinity. However, the chunks are missing the critical information about the 'restricted' policy's behavior - specifically that it rejects pods that can't get preferred affinity and puts them in Terminated state. While Chunk 4 mentions that 'restricted' is one of the supported policies, none of the retrieved chunks actually describe what the restricted policy does or how it differs from best-effort. The user would understand half of the answer (best-effort behavior) but would lack the contrasting information about restricted policy needed to fully answer the question about the difference between the two.",
      "total_score": 6.0,
      "latency_ms": 19.0,
      "grading_latency_ms": 6753.838062286377
    },
    {
      "question_id": "q_004",
      "question": "Getting error about too many NUMA nodes on my server, what's the limit?",
      "expected_answer": "8 NUMA nodes by default. Use max-allowable-numa-nodes policy option to allow more, but it's not recommended",
      "needle_found": true,
      "rank": 1,
      "hit_at_1": true,
      "hit_at_5": true,
      "llm_grade": 10,
      "llm_reasoning": "The retrieved chunks contain all key facts from the expected answer. Chunk 2 explicitly states 'The maximum number of NUMA nodes that Topology Manager allows is 8' which matches the expected answer's '8 NUMA nodes by default'. Chunk 1 provides comprehensive information about the max-allowable-numa-nodes policy option, explaining that it can be enabled to allow more than 8 NUMA nodes, and includes the warning that using this option is 'not recommended' which aligns perfectly with the expected answer. Both chunks also explain the reasoning (state explosion, performance impact) behind the limitation. A user with this error could immediately understand: (1) the default limit is 8 NUMA nodes, (2) they can use the max-allowable-numa-nodes policy option to allow more, and (3) it's not recommended to do so. All necessary information to solve the problem is present.",
      "total_score": 10.0,
      "latency_ms": 26.3,
      "grading_latency_ms": 7122.115135192871
    },
    {
      "question_id": "q_005",
      "question": "How to enable topology manager on Windows nodes?",
      "expected_answer": "Enable the WindowsCPUAndMemoryAffinity feature gate and ensure the container runtime supports it",
      "needle_found": true,
      "rank": 1,
      "hit_at_1": true,
      "hit_at_5": true,
      "llm_grade": 10,
      "llm_reasoning": "Chunk 1 contains all key facts from the expected answer. It explicitly states that Topology Manager support can be enabled on Windows by using the 'WindowsCPUAndMemoryAffinity' feature gate and that it requires support in the container runtime. This directly matches both components of the expected answer: (1) enabling the WindowsCPUAndMemoryAffinity feature gate, and (2) ensuring the container runtime supports it. The user would have complete information to solve their problem using only these chunks.",
      "total_score": 10.0,
      "latency_ms": 26.5,
      "grading_latency_ms": 3902.5416374206543
    },
    {
      "question_id": "q_006",
      "question": "Which k8s version made topology manager GA/stable?",
      "expected_answer": "v1.27",
      "needle_found": true,
      "rank": 1,
      "hit_at_1": true,
      "hit_at_5": true,
      "llm_grade": 1,
      "llm_reasoning": "The expected answer is that topology manager became GA/stable in v1.27. While Chunk 4 contains a feature-state tag showing 'v1.27' and 'state=\"stable\"', this refers to 'Device plugin integration with the Topology Manager', not the topology manager itself becoming GA/stable. The chunks discuss topology manager functionality, policies, and options at various maturity levels (alpha, beta), but none explicitly state when the topology manager component itself graduated to GA/stable status. The v1.27 reference is about a specific integration feature, not the core topology manager. Without this critical version information for the topology manager's GA status, the user cannot answer their question.",
      "total_score": 1.0,
      "latency_ms": 18.0,
      "grading_latency_ms": 5868.927478790283
    },
    {
      "question_id": "q_007",
      "question": "What flag do I pass to kubelet for setting topology policy?",
      "expected_answer": "--topology-manager-policy",
      "needle_found": true,
      "rank": 4,
      "hit_at_1": false,
      "hit_at_5": true,
      "llm_grade": 3,
      "llm_reasoning": "The expected answer is the specific flag '--topology-manager-policy'. While the chunks discuss the Topology Manager extensively (Chunk 1 mentions it's a Kubelet component, Chunk 3 discusses topology management policies, Chunk 4 discusses topology manager scopes), none of them actually provide the specific kubelet flag needed to set the topology policy. Chunk 2 mentions '--hostname-override' as an example of a kubelet flag, and Chunk 5 discusses kubelet configuration patterns, but the actual '--topology-manager-policy' flag is never mentioned. The chunks are related to the topic but don't contain the actionable information the user needs to answer their specific question about which flag to pass.",
      "total_score": 2.55,
      "latency_ms": 24.0,
      "grading_latency_ms": 5164.289712905884
    },
    {
      "question_id": "q_008",
      "question": "My latency-sensitive app is slow, containers seem to be on different NUMA nodes. How to fix?",
      "expected_answer": "Use pod scope with single-numa-node policy to place all containers on a single NUMA node, eliminating inter-NUMA communication overhead",
      "needle_found": true,
      "rank": 2,
      "hit_at_1": false,
      "hit_at_5": true,
      "llm_grade": 10,
      "llm_reasoning": "Chunk 2 contains all key facts from the expected answer. It explicitly states: 'Using the `pod` scope in tandem with `single-numa-node` Topology Manager policy is specifically valuable for workloads that are latency sensitive or for high-throughput applications that perform IPC. By combining both options, you are able to place all containers in a pod onto a single NUMA node; hence, the inter-NUMA communication overhead can be eliminated for that pod.' This directly addresses: (1) the latency-sensitive workload mentioned in the question, (2) the solution of using pod scope with single-numa-node policy, (3) placing all containers on a single NUMA node, and (4) eliminating inter-NUMA communication overhead. The user has complete information to solve their problem by configuring topologyManagerScope to 'pod' and using the single-numa-node policy.",
      "total_score": 9.5,
      "latency_ms": 31.3,
      "grading_latency_ms": 7009.645462036133
    },
    {
      "question_id": "q_009",
      "question": "Why would I use pod scope instead of container scope for topology?",
      "expected_answer": "Pod scope groups all containers to a common set of NUMA nodes, treating the pod as a whole. Container scope does separate alignment per container with no grouping, which can result in containers on different NUMA nodes",
      "needle_found": true,
      "rank": 1,
      "hit_at_1": true,
      "hit_at_5": true,
      "llm_grade": 10,
      "llm_reasoning": "The retrieved chunks contain all key facts from the expected answer. Chunk 1 explicitly states that pod scope 'allows for grouping all containers in a pod to a common set of NUMA nodes' and 'treats a pod as a whole'. Chunk 2 directly explains container scope performs 'a separate alignment' for each container with 'no notion of grouping the containers to a specific set of NUMA nodes' and that the Topology Manager performs 'an arbitrary alignment of individual containers to NUMA nodes'. These chunks provide complete information to understand why pod scope groups containers together to common NUMA nodes while container scope does separate alignment per container without grouping, which can result in containers on different NUMA nodes. The user can fully answer their question using only these chunks.",
      "total_score": 10.0,
      "latency_ms": 25.1,
      "grading_latency_ms": 7495.24712562561
    },
    {
      "question_id": "q_010",
      "question": "What are hint providers in topology manager?",
      "expected_answer": "Components like CPU Manager and Device Manager that send and receive topology information through the Topology Manager interface",
      "needle_found": true,
      "rank": 1,
      "hit_at_1": true,
      "hit_at_5": true,
      "llm_grade": 10,
      "llm_reasoning": "The retrieved chunks contain all key facts from the expected answer. Chunk 1 explicitly defines Hint Providers as 'components...to send and receive topology information' and specifically identifies 'CPU and Device Manager' as examples of Hint Providers that interact through the Topology Manager interface. The chunk states 'The Topology Manager provides an interface for components, called *Hint Providers*, to send and receive topology information' and 'The Topology Manager receives topology information from the *Hint Providers*'. This directly matches the expected answer's definition. Additionally, Chunks 2 and 4 reinforce this by showing how CPU Manager and Device Manager act as Hint Providers in practice, consulting with Topology Manager to provide topology hints for resource allocation decisions.",
      "total_score": 10.0,
      "latency_ms": 16.7,
      "grading_latency_ms": 4542.388200759888
    },
    {
      "question_id": "q_011",
      "question": "How do I configure kubelet to prefer NUMA nodes that are closer together?",
      "expected_answer": "Add prefer-closest-numa-nodes=true to Topology Manager policy options. This makes best-effort and restricted policies favor NUMA nodes with shorter distance",
      "needle_found": true,
      "rank": 3,
      "hit_at_1": false,
      "hit_at_5": true,
      "llm_grade": 1,
      "llm_reasoning": "The expected answer states that to configure kubelet to prefer NUMA nodes that are closer together, you need to add 'prefer-closest-numa-nodes=true' to Topology Manager policy options, which makes best-effort and restricted policies favor NUMA nodes with shorter distance. None of the retrieved chunks mention 'prefer-closest-numa-nodes' policy option at all. Chunk 1 discusses device plugin integration with Topology Manager. Chunk 2 is about troubleshooting topology management. Chunk 3 discusses 'pod' scope configuration. Chunk 4 discusses 'max-allowable-numa-nodes' policy option (a different option). Chunk 5 discusses reserved memory configuration. While all chunks are related to NUMA and topology management, none contain the specific configuration option or mechanism described in the expected answer. A user would not be able to solve their problem using these chunks.",
      "total_score": 0.95,
      "latency_ms": 27.0,
      "grading_latency_ms": 6057.622671127319
    },
    {
      "question_id": "q_012",
      "question": "When did prefer-closest-numa-nodes become generally available?",
      "expected_answer": "Kubernetes 1.32",
      "needle_found": true,
      "rank": 1,
      "hit_at_1": true,
      "hit_at_5": true,
      "llm_grade": 10,
      "llm_reasoning": "Chunk 1 directly and completely answers the question. It explicitly states 'The `prefer-closest-numa-nodes` option is GA since Kubernetes 1.32', which matches the expected answer exactly. The chunk provides the precise version (1.32) and the status (GA/generally available) that the user is asking about. No additional information is needed to answer this question.",
      "total_score": 10.0,
      "latency_ms": 15.1,
      "grading_latency_ms": 3882.012128829956
    },
    {
      "question_id": "q_013",
      "question": "What's the default topology manager policy if I don't set anything?",
      "expected_answer": "none - which does not perform any topology alignment",
      "needle_found": true,
      "rank": 1,
      "hit_at_1": true,
      "hit_at_5": true,
      "llm_grade": 10,
      "llm_reasoning": "The chunks contain all key facts from the expected answer. Chunk 2 explicitly states 'This is the default policy and does not perform any topology alignment' under the '`none` policy {#policy-none}' section. Chunk 4 lists the four supported policies and clearly indicates '`none` (default)'. These two chunks together provide complete information: (1) that 'none' is the default topology manager policy, and (2) that this policy does not perform any topology alignment. The user can definitively answer their question using only these retrieved chunks.",
      "total_score": 10.0,
      "latency_ms": 24.9,
      "grading_latency_ms": 5081.0301303863525
    },
    {
      "question_id": "q_014",
      "question": "Pod was scheduled but then failed on the node, says something about topology. Is that a bug?",
      "expected_answer": "No, this is a known limitation. The scheduler is not topology-aware, so it's possible to be scheduled on a node and then fail due to the Topology Manager",
      "needle_found": false,
      "rank": null,
      "hit_at_1": false,
      "hit_at_5": false,
      "llm_grade": 2,
      "llm_reasoning": "The expected answer contains two critical facts: (1) this is a known limitation, not a bug, and (2) the scheduler is not topology-aware, so pods can be scheduled on a node but then fail due to the Topology Manager. The retrieved chunks discuss various pod scheduling and failure scenarios, including general pod failures (Chunk 1), pod status fields (Chunk 2), pre-scheduled pods (Chunk 3), pending pods (Chunk 4), and topology spread constraints (Chunk 5). However, none of the chunks mention the Topology Manager, explain that the scheduler lacks topology awareness in the context of Topology Manager constraints, or clarify that this specific scenario (scheduling success followed by topology-related node failure) is a known limitation rather than a bug. Chunk 5 discusses topologySpreadConstraints but this is about pod distribution across topology domains, not about the Topology Manager component that enforces hardware topology requirements. The chunks are tangentially related to pod scheduling but completely miss the specific issue the user is asking about.",
      "total_score": 1.2,
      "latency_ms": 39.3,
      "grading_latency_ms": 6797.062397003174
    },
    {
      "question_id": "q_015",
      "question": "What QoS class does my pod need to be for topology hints to work?",
      "expected_answer": "Topology Manager aligns Pods of all QoS classes (BestEffort, Burstable, Guaranteed)",
      "needle_found": true,
      "rank": 1,
      "hit_at_1": true,
      "hit_at_5": true,
      "llm_grade": 2,
      "llm_reasoning": "The user is asking about QoS class requirements for 'topology hints' to work. The expected answer states that Topology Manager aligns Pods of all QoS classes (BestEffort, Burstable, Guaranteed). While Chunks 1 and 2 extensively discuss how Topology Manager interacts with different QoS classes and provides topology hints, they do NOT state that topology hints work for all QoS classes. In fact, Chunk 2 explicitly states 'For pods in a QoS class other than Guaranteed, the Memory Manager provides default topology hints' which could mislead users into thinking only Guaranteed pods get proper topology hints. Chunk 5 mentions 'Topology Aware Routing' and 'Topology Hints' but in the context of Services, not Pods and QoS classes. The chunks discuss related concepts but do not answer the core question: that topology hints work for pods of ALL QoS classes. The user would not be able to confidently answer their question from these chunks alone.",
      "total_score": 2.0,
      "latency_ms": 27.6,
      "grading_latency_ms": 6170.652151107788
    },
    {
      "question_id": "q_016",
      "question": "How to set topology scope to pod level in kubelet?",
      "expected_answer": "Set topologyManagerScope to pod in the kubelet configuration file",
      "needle_found": true,
      "rank": 1,
      "hit_at_1": true,
      "hit_at_5": true,
      "llm_grade": 10,
      "llm_reasoning": "The retrieved chunks contain complete and exact information to answer the question. Chunk 1 explicitly states 'To select the pod scope, set topologyManagerScope in the kubelet configuration file to pod', which directly matches the expected answer. Chunk 2 provides additional context about topology manager scopes and confirms that topologyManagerScope can be set in the kubelet configuration file. The chunks provide both the exact parameter name (topologyManagerScope), the exact value to set (pod), and the exact location (kubelet configuration file). All key facts from the expected answer are present and actionable.",
      "total_score": 10.0,
      "latency_ms": 25.1,
      "grading_latency_ms": 5274.679899215698
    },
    {
      "question_id": "q_017",
      "question": "What feature gate do I need for topology manager policy options?",
      "expected_answer": "TopologyManagerPolicyOptions (enabled by default)",
      "needle_found": true,
      "rank": 1,
      "hit_at_1": true,
      "hit_at_5": true,
      "llm_grade": 10,
      "llm_reasoning": "The retrieved chunks contain all key facts from the expected answer. Chunk 1 explicitly states: 'Support for the Topology Manager policy options requires `TopologyManagerPolicyOptions` [feature gate] to be enabled (it is enabled by default).' This directly answers the user's question with both the feature gate name (TopologyManagerPolicyOptions) and its default state (enabled by default). Additionally, Chunk 1 provides supplementary information about related feature gates (TopologyManagerPolicyBetaOptions and TopologyManagerPolicyAlphaOptions) that control groups of policy options at different maturity levels. The user would have complete and actionable information to answer their question using only these chunks.",
      "total_score": 10.0,
      "latency_ms": 22.5,
      "grading_latency_ms": 4405.45392036438
    },
    {
      "question_id": "q_018",
      "question": "My multi-socket server has GPUs and CPUs, how does k8s coordinate their placement?",
      "expected_answer": "Topology Manager coordinates CPU Manager and Device Manager to make topology-aligned resource allocation decisions, avoiding CPUs and devices being allocated from different NUMA nodes",
      "needle_found": false,
      "rank": null,
      "hit_at_1": false,
      "hit_at_5": false,
      "llm_grade": 7,
      "llm_reasoning": "The chunks contain relevant information about topology management and CPU/GPU coordination, but lack the specific detail from the expected answer. Chunk 1 mentions Topology Manager as 'a kubelet component that aims to coordinate the set of components responsible for optimizations' and references CPU Manager policies that consider topology. However, the chunks don't explicitly state that Topology Manager coordinates CPU Manager AND Device Manager together, nor do they clearly explain that this prevents CPUs and devices from being allocated from different NUMA nodes. The GPU scheduling chunks (2-3) explain device plugins but don't mention topology coordination. Chunk 4 discusses NUMA-aware CPU distribution options but doesn't connect this to GPU placement. A user could infer that topology management exists and relates to CPU/device placement, but the specific coordination mechanism between CPU Manager and Device Manager for NUMA-aligned allocation is not clearly stated.",
      "total_score": 4.2,
      "latency_ms": 26.8,
      "grading_latency_ms": 7179.178237915039
    },
    {
      "question_id": "q_019",
      "question": "With single-numa-node policy, when exactly does a pod get rejected?",
      "expected_answer": "When a single NUMA Node affinity is not possible - if more than one NUMA node is required to satisfy the allocation, the pod is rejected",
      "needle_found": true,
      "rank": 2,
      "hit_at_1": false,
      "hit_at_5": true,
      "llm_grade": 10,
      "llm_reasoning": "Chunk 2 directly and completely answers the question. It states that with single-numa-node policy, the Topology Manager determines if a single NUMA Node affinity is possible, and 'If, however, this is not possible then the Topology Manager will reject the pod from the node.' This matches the expected answer exactly - the pod is rejected when a single NUMA Node affinity is not possible (i.e., when more than one NUMA node would be required). The chunk also provides additional context about the resulting Terminated state and admission failure, which helps the user understand the full behavior.",
      "total_score": 9.5,
      "latency_ms": 21.2,
      "grading_latency_ms": 5250.7336139678955
    },
    {
      "question_id": "q_020",
      "question": "What happens if topology manager can't find preferred NUMA affinity with best-effort policy?",
      "expected_answer": "The pod is admitted to the node anyway - best-effort stores the non-preferred hint and allows the pod",
      "needle_found": true,
      "rank": 1,
      "hit_at_1": true,
      "hit_at_5": true,
      "llm_grade": 10,
      "llm_reasoning": "Chunk 1 directly and completely answers the question. It states that with best-effort policy, if the affinity is not preferred, 'the Topology Manager will store this and admit the pod to the node anyway.' This matches both key facts from the expected answer: (1) the pod is admitted to the node anyway, and (2) the non-preferred hint is stored. The chunk provides the exact information needed to answer what happens when topology manager can't find preferred NUMA affinity with best-effort policy.",
      "total_score": 10.0,
      "latency_ms": 25.6,
      "grading_latency_ms": 4473.451375961304
    }
  ]
}